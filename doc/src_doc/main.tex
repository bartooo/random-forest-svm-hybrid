%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Bachelor's & Master's Thesis Template             %%
%% Copyleft by Artur M. Brodzki & Piotr Woźniak      %%
%% Covered for Lab/Assigment report by Tymon Żarski  %%
%% Faculty of Electronics and Information Technology %%
%% Warsaw University of Technology, 2019-2020        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\documentclass[
    left=2.5cm,         % Sadly, generic margin parameter
    right=2.5cm,        % doesnt't work, as it is
    top=2.5cm,          % superseded by more specific
    bottom=3cm,         % left...bottom parameters.
    bindingoffset=6mm,  % Optional binding offset.
    nohyphenation=false % You may turn off hyphenation, if don't like.
]{eiti/eiti-report}

\langpol % Dla języka angielskiego mamy \langeng
\graphicspath{{img/}}             % Katalog z obrazkami.
\addbibresource{bibliografia.bib} % Plik .bib z bibliografią
\usepackage{forest}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings} % Code listings, with syntax highlighting
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{rotating}
\usepackage{makecell}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\argmin}{arg\,min}


\begin{document}

%--------------------------------------
% Strona tytułowa
%--------------------------------------
\instytut{XXXXXX}
\przedmiot{Uczenie Maszynowe - Projekt}
\specjalnosc{Sztuczna Inteligencja}
\title{
    Połączenie Lasu Losowego ID3 z Maszyną Wektorów\\
    Nośnych (SVM) w zadaniu klasyfikacji binarnej
}
\reportDescription{
    Dokumentacja
}

\author{Bartosz Cywiński, Łukasz Staniszewski}
\album{304025, 304098}

\prowadzacy{dr Rafał Biedrzycki}
\date{\today}
\maketitle

%--------------------------------------
% Spis treści
%--------------------------------------
\tableofcontents

%--------------------------------------
% Rozdziały
%--------------------------------------
\cleardoublepage % Zaczynamy od nieparzystej strony
\pagestyle{headings}

\newpage % Rozdziały zaczynamy od nowej strony
\section{Temat projektu}

Celem projektu była implementacja połączenia lasu losowego z SVM w zadaniu klasyfikacji binarnej. Wykonana została własna implementacja obu algorytmów na podstawie źródeł \cite{wsisvm},  \cite{umasvm} oraz \cite{eslII}. Najistotniejszą modyfikacją w stosunku do klasycznego algorytmu lasu losowego było to, że klasyfikatorem w lesie jest zarówno drzewo decyzyjne ID3 jak i SVM, zakładając że jest taka sama liczność obu klasyfikatorów. Proces inferencji w lesie losowym jest niezmienny, a więc ostateczną predykcją dla danego przykładu jest najliczniej przewidywana klasa przez wszystkie klasyfikatory. 
%Zadaniem rozwiązywanym przez taki las losowy będzie klasyfikacja binarna.

\section{Drzewo decyzyjne}
Pierwszym z klasyfikatorów użytych w zmodyfikowanym lesie losowym jest drzewo decyzyjne. Algorytmem uczenia drzewa jest algorytm ID3. Drzewo decyzyjne zaimplementowane zostało w sposób uniwersalny na tyle, żeby poprawnie działało zarówno dla klasyfikacji binarnej jak i wieloklasowej.

\subsection{Opis algorytmu}


\begin{algorithm}
\caption{ID3}\label{alg:cap}
\hspace*{\algorithmicindent} \textbf{Input:} \begin{math}\mathit{S}\end{math}: zbiór par uczących, \begin{math}\mathit{Y}\end{math}: zbiór klas, $D$: zbiór atrybutów wejściowych, \begin{math}\mathit{d}\end{math}: obecna głębokość drzewa
\begin{algorithmic}[1]
\If {\texttt{\begin{math}S=\emptyset\end{math}}}
    \Return
\EndIf\\

\If {wszystkie przykłady należą do klasy $y$}\\
    \Return {Liść z klasą $y$}
\EndIf\\


\State{\begin{math} (j,t)=\argmin_{j,t}{H(S)} \end{math}}
\State{\texttt{Root} = węzeł zbudowany na zbiorze $S$}
\If{!stop}
    \State{\texttt{Root.left = ID3}($S_{-}$, $Y$, $D$, $d+1$)}
    \State{\texttt{Root.right = ID3}($S_{+}$, $Y$, $D$, $d+1$)}
\EndIf \\
\Return{\texttt{Root}}

\end{algorithmic}
\end{algorithm}

Drzewo decyzyjne ma strukturę drzewa binarnego. W każdym węźle, podczas konstruowania drzewa, wyszukiwany jest atrybut w zbiorze wszystkich atrybutów zbioru danych oraz jego konkretna wartość, która podzieli zbiór danych na dwa podzbiory w taki sposób, aby suma entropii podzbiorów była jak najmniejsza. Na podstawie powstałych w wyniku operacji podziału podzbiorów rekurencyjnie tworzone są kolejne węzły drzewa decyzyjnego.

Drzewo decyzyjne ma następujące atrybuty:
\begin{enumerate}
    \item maksymalna głębokość drzewa
    \item minimalna różnica między entropią po podziale, a entropią przed podziałem
    \item minimalna liczba przykładów w węźle drzewa
\end{enumerate}

Oznaczając zbiór przykładów z etykietami jako $S$, na początku procesu uczenia drzewa decyzyjnego, drzewo ma tylko jeden węzeł (korzeń), który zawiera wszystkie przykłady ze zbioru $S$. Kolejno wskutek wywołania algorytmu ID3 rozpoczyna się właściwy proces uczenia drzewa, opisany przedstawionym powyżej pseudokodem.\\

\textit{Linie 1-2:}
Jeśli w zbiorze $S$, który był argumentem algorytmu ID3, nie ma już żadnej pary uczącej, proces dalszego uczenia drzewa należy zatrzymać.\\

\textit{Linie 4-6:}
Jeśli w zbiorze $S$ znajdują się przykłady należące tylko do jednej klasy, to znaczy że zbiór jest już idealnie podzielony i nie należy kontynuować procesu uczenia, więc zwracany jest liść drzewa, którego predykcja jest jedyną klasą w zbiorze $S$.\\

\textit{Linia 8:}
Oznaczając zbiór wszystkich atrybutów w zbiorze danych jako $D$, dla każdego indeksu atrybutu $j=0,...,D-1$ oraz dla każdej wartości atrybutu występującej w zbiorze danych $t$ wykonywane są następujące kroki:
\begin{enumerate}
  \item Zbiór wszystkich par uczących $S$ dzielony jest na dwa podzbiory: \begin{math}S_{-}=\{(x,y)|(x,y)\in{S}, x^{(j)}<t\}, S_{+}=\{(x,y)|(x,y)\in{S}, x^{(j)}\ge{t}\}\end{math}.
  \item Oceniana jest jakość podziału. W tym celu liczona jest entropia podziału jako entropia ważona dwóch zbiorów: \begin{math}H(S_{-}, S_{+})=\frac{|S_{-}|}{|S|}H(S_{-})+\frac{|S_{+}|}{|S|}H(S_{+})\end{math}, przy czym wartość entropii $H$ dla zbioru $S$ definiuje się jako: \begin{math}
      H(S)=\sum_{c\in{C}}{-p(c)\log_2p(c)}
  \end{math}, gdzie $C$ to zbiór wszystkich klas obecnych w zbiorze $S$, a $p(c)$ to stosunek liczby przykładów klasy $c$ w zbiorze $S$ do liczby wszystkich przykładów w zbiorze $S$. \\
\end{enumerate}


Wykonując powyższe kroki znajdywany jest taki atrybut $j$ oraz taka jego wartość $t$ dla których entropia jest najniższa.\\

\textit{Linia 10:}
Aby dokonać podziału węzła spełnione muszą być następujące warunki:
\begin{enumerate}
    \item Nie może być przekroczona maksymalna głębokość drzewa.
    \item Różnica między entropią $H(S)$, a entropią ważoną $H(S_{-}, S_{+})$ musi być większa od minimalnej dopuszczalnej różnicy między wartościami entropii.
    \item Liczność, zarówno zbioru $S_{-}$, jak i zbioru $S_{+}$ musi być większa od minimalnej dopuszczalnej liczby przykładów w węźle drzewa.
\end{enumerate}
W przypadku niespełnienia jakiegokolwiek z powyższych warunków dany węzeł drzewa nie zostanie dalej podzielony.\\

\textit{Linie 11-12:}
Do obecnego korzenia (węzła Root), przypisywane są węzły dzieci. Węzły te tworzone są przez rekursywne wywołanie algorytmu ID3, kolejno dla podzbiorów $S_{-}$, jak i $S_{+}$, jednocześnie zwiększając obecną głębokość drzewa o 1.

\subsubsection{Przykładowe obliczenia przy podziale zbioru danych:}
Zakładając, że: 
\begin{gather*}
    S = \{([0, 2, 5], 0), ([0, 4, 6], 1), ([0, -1, 7], 0)\}
\end{gather*}
Analizując ten prosty zbiór danych można zauważyć, że atrybut o indeksie $j = 1$ oraz jego wartość $t = 4$ podzieli zbiór $S$ tworząc podzbiory $S_{-}$ oraz $S_{+}$ w sposób następujący:
\begin{gather*}
    S_{-} = \{(x,y)|(x,y)\in{S}, x^{(1)}<4\} = \{([0, 2, 5], 0), ([0, -1, 7], 0)\},\\
    S_{+} = \{(x,y)|(x,y)\in{S}, x^{(1)}\ge{4}\} = \{([0, 4, 6], 1)\}
\end{gather*}
Zatem licząc entropie poszczególnych zbiorów:
\begin{gather*}
H(S_{-})=-\frac{2}{2}\log_2{\frac{2}{2}} + (-\frac{0}{2}\log_2{\frac{0}{2}}) = 0 + 0 = 0\\
H(S_{+})=-\frac{0}{1}\log_2{\frac{0}{2}} + (-\frac{1}{1}\log_2{\frac{1}{1}}) = 0 + 0 = 0\\
H(S_{-}, S_{+})=\frac{2}{3}H(S_{-})+\frac{1}{3}H(S_{+})  = \frac{2}{3}\cdot0+\frac{1}{3}\cdot0 = 0
\end{gather*}
Widać na tym przykładzie, że entropia podziału zbioru $S$ jest równa jej minimalnej możliwej wartości, bo podzbiory $S_{-}$ i $S_{+}$ idealnie podzieliły zbiór $S$ pod względem klas. Dla odróżnienia, gdyby został wybrany ten sam atrybut o indeksie $j = 1$, ale o wartości $t = 2$, podział wyglądałby następująco:
\begin{gather*}
    S_{-} = \{(x,y)|(x,y)\in{S}, x^{(1)}<2\} = \{([0, -1, 7], 0)\},\\
    S_{+} = \{(x,y)|(x,y)\in{S}, x^{(1)}\ge{2}\} = \{([0, 2, 5], 0), ([0, 4, 6], 1)\}
\end{gather*}
Ponownie licząc entropie poszczególnych zbiorów:
\begin{gather*}
H(S_{-})=-\frac{0}{1}\log_2{\frac{0}{1}} + (-\frac{1}{1}\log_2{\frac{1}{1}}) = 0 + 0 = 0\\
H(S_{+})=-\frac{1}{2}\log_2{\frac{1}{2}} + (-\frac{1}{2}\log_2{\frac{1}{2}}) = \frac{1}{2} + \frac{1}{2} = 1\\
H(S_{-}, S_{+})=\frac{1}{3}H(S_{-})+\frac{2}{3}H(S_{+})  = \frac{1}{3}\cdot0+\frac{2}{3}\cdot{1} = \frac{2}{3}
\end{gather*}
Co pokazuje, że gdy zbiory byłyby tak podzielone, entropia miałaby większą wartość, dlatego też podział nie zostałby wybrany jako najlepszy możliwy.


\subsection{Predykcje algorytmu}
Po całkowitym wykonaniu się algorytmu uczącego drzewo ID3, drzewo jest w pełni zbudowane i można na nim wykonywać predykcje. Dla pojedynczej próbki danych algorytm przechodzi od korzenia do liścia, w każdym węźle wybierając dziecko do którego powinien następnie przejść na podstawie wartości atrybutu podziału danego węzła dla próbki danych. Mianowicie, przyjmując za $j$ indeks atrybutu podziału danego węzła, a za $t$ przyjmując wartość tego atrybutu, jeśli dla próbki danych $x$: \begin{math}
    x^{(j)} < t
\end{math}, to algorytm przechodzi do lewego dziecka. W przeciwnym przypadku, algorytm przejdzie do prawego dziecka. Po dotarciu do liścia drzewa decyzyjnego, jako predykcja zwracana jest klasa większościowa danego liścia.


\section{Algorytm SVM}
Drugim z klasyfikatorów użytych w implementacji jest Maszyna Wektorów Nośnych (SVM) dopuszczająca pomyłki. SVM zaimplementowany został dla przypadku binarnego ze zbiorem klas $Y=\{-1,1\}$, a także, dla ułatwienia implementacji, założona została wersja algorytmu bez przekształcenia jądrowego (bazowe jądro liniowe).

\subsection{Opis algorytmu}
Zadanie polega na znalezieniu funkcji rozgraniczającej $ f(x)={x\cdot w-b} $, która tworzy hiperpłaszczyznę zapewniającą klasyfikację. Otrzymana funkcja powinna zapewniać jak najmniejszą liczbę pomyłek przy klasyfikowaniu elementów zbioru wejściowego do odpowiedniej klasy.


Klasyfikacja odbywa się poprzez zwrócenie dla danego zestawu cech $x$ klasy $y(x) = -1 $ lub $ y(x) = 1$, do której przynależność wynika z następującej zależności:
\begin{equation}
y(x) =
{
\left\{
\begin{array}{ll}
-1 & \textrm{, $f(x) \leq 0$}\\
1 & \textrm{, $f(x) > 0$}
\end{array}
\right
}
\end{equation}
Na końcu procesu inferencji, otrzymane predykcje są mapowane z powrotem do odpowiednich klas ze zbioru $X$, przykładowo: $-1 \xrightarrow{} 0 $.
\\
Ze względu na trenowanie dopuszczające pomyłki, aby otrzymać wyżej wymienioną funkcję $f$, należy znaleźć parametry $(w,b)$, minimalizujące funkcję straty $J$:
\begin{align}
    (w,b)=\argmin_{w,b} J(w,b)\\
    J(w,b)=\Sigma_i\cdot \xi_i + \lambda\cdot ||w||^2
\end{align}
\newpage
Przy czym, istotne są odpowiednie ograniczenia (gdzie $\xi_i$ oznacza stratę dla $i$-tego przykładu trenującego, jeśli klasyfikacja jest błędna; a $\lambda$ decyduje o istotności szerokości regionu separującego):
\begin{align}
    \lambda > 0 \\
    \forall_i \;[ \xi_i \ge 0 \:\land\: y_i\cdot(x_i\cdot w-b) \ge 1-\xi_i ]
\end{align}

Wymienione wyżej warunki (4) oraz (5) w połączeniu z postacią funkcji straty (3) implikują ostateczną postać funkcji $J$:
\begin{equation}
    J(w,b)=\Sigma_i\max(1-f(x_i)\cdot y_i, 0) + \lambda\cdot||w||^2
\end{equation}

\\
Powyższy opis algorytmu można podsumować w postaci algorytmu uczenia (Algorithm 2) oraz algorytmu predykcji (Algorithm 3). W algorytmie predykcji zastosowane zostało wyrażenie logiczne, zwracające $1$ gdy jest prawdziwe, a $0$ gdy fałszywe, co pozwala mu zwracać numery klas zamiast liczb rzeczywistych. Na końcu, zwracane klasy mapowane są do odpowiednich wartości, przykładowo: $-1 \xrightarrow{} 0 $. W algorytmie uczenia, początkowo poprawiane są wejściowe etykiety tak, aby były ze zbioru $\{-1,1\}$, następnie inicjalizowane są parametry modelu i przekazywane do metody optymalizacyjnej, której opis znajduje się w następnym podrozdziale. Na końcu zwracane są wytrenowane parametry modelu. 

\begin{algorithm}
\caption{Uczenie SVM}\label{alg:cap}
\hspace*{\algorithmicindent} \textbf{Input:} \begin{math}\mathit{X}\end{math}: zestaw przykładów dla zbioru trenującego, \begin{math}\mathit{Y}\end{math}: zestaw etykiet dla zbioru trenującego, 
\begin{math}\lambda\end{math}: parametr funkcji straty, \begin{math}\mathit{V}\end{math}: wektor parametrów dla optymalizatora
\begin{algorithmic}[1]
\State{\begin{math} Y'= \end{math} \texttt{correct\_targets}(\begin{math}Y \end{math})}
\State{\texttt{initialize:}\begin{math} w_0 = [0\;0\;...\;0]^T \end{math} \texttt{, }\begin{math} b_0 = 0 \end{math}}
\State{\begin{math}(w,b)=\end{math}\texttt{ gradient\_descent}(\begin{math}w_0, b_0,X,Y',V,\lambda\end{math})}
\State{\Return \begin{math}(w,b)\end{math}}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Predykcja SVM}\label{alg:cap}
\hspace*{\algorithmicindent} \textbf{Input:} \begin{math}\mathit{X}\end{math}: zestaw przykładów dla zbioru ewaluacyjnego,
\begin{math}\mathit{(w,b)}\end{math}: parametry modelu,
\begin{math}\lambda\end{math}: parametr funkcji straty
\begin{algorithmic}[1]
\State{\Return \texttt{repair\_targets}(\begin{math}2\cdot((X\cdot w - b) > 0) -1\end{math})}
\end{algorithmic}
\end{algorithm}

\subsection{Optymalizacja}
Istotnym elementem uczenia modelu SVM, jest wyznaczenie jego parametrów poprzez minimalizację funkcji straty $J$. Zaimplementowany został algorytm Stochastycznego Spadku Gradientowego (Stochastic Gradient Descent / SGD), do którego działania wymagane jest obliczenie gradientu funkcji straty $J(w,b)$ po parametrach modelu $w$ oraz $b$.
\begin{equation}
\nabla J =
\begin{bmatrix}
    \partial J \over \partial w_1 &
    \hdots &
    \partial J \over \partial w_n &
    \partial J \over \partial b
\end{bmatrix}^T
\end{equation}

Gradient ten jest wektorem pochodnych cząstkowych wyliczanych po kolejnych parametrach modelu (gdzie $x_{k[i]}$ oznacza $i$-ty atrybut $k$-tego przykładu).

\begin{align}
    {\partial J \over \partial w_i}=
    {\lambda \cdot 2 \cdot w_i} + \Sigma_k(1 \cdot
    {\left\{ \begin{array}{ll}
        0 & \textrm{, $ 1-f(x_k\cdot )y_k \leq 0$ }\\
        -y_k \cdot x_{k[i]} & \textrm{, $ 1-f(x_k)\cdot y_k > 0$}
    \end{array}\right })
    \\
    {\partial J \over \partial b}=
    \Sigma_k (1 \cdot {
        \left\{ \begin{array}{ll}
            0 & \textrm{, $ 1-f(x_k) \cdot y_k \leq 0$ }\\
            y_k & \textrm{, $ 1-f(x_k) \cdot y_k > 0$}
        \end{array}\right
    }
    )
\end{align}

Zaimplementowany optymalizator SGD można przedstawić w formie algorytmu (Algorithm 4). Metoda zakłada jednokrotne przetworzenie całego zbioru trenującego w ramach jednego kroku i rozpoczyna się od ustalenia parametrów optymalizatora, gdzie $max\_steps$ oznacza maksymalną liczbę kroków optymalizacji, $min\_steps$ oznacza najmniejszą możliwą normę z parametrów modelu, natomiast $\beta$ to tzw. learning rate.

\begin{algorithm}
\caption{SGD}\label{alg:cap}
\hspace*{\algorithmicindent} \textbf{Input:} \begin{math}(\mathit{w}_0,\mathit{b}_0)\end{math}: zainicjowane parametry modelu, \begin{math}\mathit{X}\end{math}: zbiór przykładów (wejść), $Y$: zbiór etykiet (wyjść), \begin{math}\mathit{V}\end{math}: parametry optymalizatora, \begin{math}\lambda\end{math}: współczynnik \begin{math}\lambda\end{math} modelu
\begin{algorithmic}[1]
\State{\begin{math} (max\_steps,\beta,min\_eps)=\mathit{V}\end{math}}
\State{\begin{math} (\mathit{w},\mathit{b})=(\mathit{w}_0,\mathit{b}_0)\end{math}}
\State{\begin{math} step=0\end{math}}
\While {\begin{math} step\leq max\_steps\end{math}}
	\State{
	    \begin{math} \nabla w= 
	        {\lambda \cdot 2 \cdot w_i} + \Sigma_k(1 \cdot
	        {\left\{\begin{array}{ll}
                0 & \textrm{, $ 1-f(x_k)\cdot y_k \leq 0$ }\\
                -y_k \cdot x_{k[i]} & \textrm{, $ 1-f(x_k)\cdot y_k > 0$}
            \end{array}\right })
        \end{math}
        \texttt{ for each }
        \begin{math}i=1...M\end{math}
    }
    \State{
        \begin{math} \nabla b=
            \Sigma_k (1 \cdot {
            \left\{ \begin{array}{ll}
                0 & \textrm{, $ 1-f(x_k) \cdot y_k \leq 0$ }\\
                y_k & \textrm{, $ 1-f(x_k) \cdot y_k > 0$}
            \end{array}\right })
        \end{math}
    }
    \If{\begin{math}|\mathit{w}| < min\_eps\end{math}}
        \Return \begin{math} (\mathit{w},\mathit{b}) \end{math}
	\EndIf
    \State{\begin{math} \mathit{w} = \mathit{w} - \beta \cdot \nabla \mathit{w} \end{math}}
    \State{\begin{math} \mathit{b} = \mathit{w} - \beta \cdot \nabla \mathit{b} \end{math}}
    \State{\begin{math} step = step + 1 \end{math}}
\EndWhile
\State{\Return \begin{math} (\mathit{w},\mathit{b}) \end{math}}

\end{algorithmic}
\end{algorithm}

\subsection{Przykładowe obliczenia}
Zakładając, że z każdym przykładem związane są 3 atrybuty i istnieje następujący zbiór treningowy $T$  składający się z 3 przykładów oraz zbiór ewaluacyjny $E$ składający się z 2 przykładów:
\begin{gather*}
    X_T= \begin{vmatrix}
        1&2&3\\
        6&5&4\\
        7&8&7\\
    \end{vmatrix},
    Y_T= \begin{vmatrix}
        1\\
        0\\
        1\\
    \end{vmatrix}, 
    X_E= \begin{vmatrix}
        -3&4&1\\
        4&2&12\\
    \end{vmatrix}
\end{gather*}
Na początku konieczne jest przerobienie etykiet klas dla przykładów trenujących oraz inicjalizacja modelu:
\begin{gather*}
    Y'_T= \begin{vmatrix}
        1\\
        \pmb{-1}\\
        1
    \end{vmatrix}, 
    w = \begin{vmatrix}
        1\\
        -1\\
        1
    \end{vmatrix},  b = 3
\end{gather*}
Przy założeniu, że $max\_steps=1$, $\lambda = 0.5$ oraz $\beta = 0.2$, wykonywany jest jeden krok SGD:
\begin{gather*}
    1-f(X_T)\cdot Y'_T = \begin{vmatrix}
        2\\
        3\\
        -2
    \end{vmatrix} \implies
    \nabla w = \begin{vmatrix}
        0.5 \cdot 2 \cdot 1 + \texttt{sum(}\begin{vmatrix}
        -1 &
        6 &
        0
    \end{vmatrix}^T\texttt{)}\\
        0.5 \cdot 2 \cdot -1 + \texttt{sum(}\begin{vmatrix}
        -2 &
        5 &
        0
    \end{vmatrix}^T\texttt{)}\\
        0.5 \cdot 2 \cdot 1 + \texttt{sum(}\begin{vmatrix}
        -3 &
        4 &
        0
    \end{vmatrix}^T\texttt{)}
    \end{vmatrix} = \begin{vmatrix}
        6\\
        2\\
        2
    \end{vmatrix}\\
    1-f(X_T)\cdot Y'_T = \begin{vmatrix}
        2 &
        3 &
        -2
    \end{vmatrix}^T \implies
    \nabla b = \texttt{sum(}\begin{vmatrix}
        1 &
        -1 &
        0
    \end{vmatrix}^T\texttt{)} = 0
\end{gather*}
Na końcu uczenia ustalane są ostateczne parametry modelu:
\begin{gather*}
    w = 
        \begin{vmatrix}
            1\\
            -1\\
            1
        \end{vmatrix} - 0.2 \cdot \begin{vmatrix}
            6\\
            2\\
            2
        \end{vmatrix} = \begin{vmatrix}
            -0.2\\
            -1.4\\
            0.6
        \end{vmatrix}
    , b =
    3 - 0.2 \cdot 0 = 3
\end{gather*}
\\
    Po ustaleniu parametrów modelu, można przejść do predykcji:
\begin{gather*}
    Y_E = 2\cdot (
            (\begin{vmatrix}
                -3&4&1\\
                4&2&12\\
            \end{vmatrix}
            \cdot 
            \begin{vmatrix}
                -0.2\\
                -1.4\\
                0.6
            \end{vmatrix} 
            - 3
            ) 
        > 0) -1 = 
        2 \cdot (
        \begin{vmatrix}
                -7.4\\
                0.6
        \end{vmatrix} 
        > 0) - 1 = 
        \begin{vmatrix}
                -1\\
                1
        \end{vmatrix} 
\end{gather*}

\section{Las losowy}
Las losowy wykorzystuje oba algorytmy opisane powyżej. Za klasycznym algorytmem lasu losowego stoi idea baggingu. W algorytmie konstruuje się wiele nieskorelowanych drzew losowych, a następnie się je uśrednia. W przypadku modyfikacji algorytmu, wykonywanego w ramach projektu, zamiast samych drzew decyzyjnych, występuje zbiór klasyfikatorów - drzew decyzyjnych oraz SVM. Każdy klasyfikator wytrenowany jest na zbiorze próbek danych losowanych z oryginalnego zbioru ze zwracaniem. Ponadto w procesie uczenia drzewa losowego przed każdym podziałem węzła losowany jest bez zwracania podzbiór indeksów atrybutów uwzględnianych w procesie podziału. Natomiast w procesie uczenia SVM, podzbiór indeksów atrybutów losowany jest raz, przed rozpoczęciem procedury treningowej. Zasadniczą ideą baggingu jest uśrednienie wielu modeli z dość małym obciążeniem, w skutek czego zmniejszana jest ich wariancja.

\subsection{Opis algorytmu}

\begin{algorithm}
\caption{Las losowy}\label{alg:cap}
\hspace*{\algorithmicindent} \textbf{Input:} \begin{math}\mathit{S}\end{math}: zbiór par uczących, \begin{math}\mathit{Y}\end{math}: zbiór klas, $D$: zbiór atrybutów wejściowych, $B$: liczba klasyfikatorów
\begin{algorithmic}[1]
\For {b = 1 to B:}
    \State{Wylosuj ze zwracaniem $|S|$ próbek danych ze zbioru $S$.}
    \If{b$\mod$ 2 == 0}
        \State{Wytrenuj drzewo decyzyjne algorytmem ID3 na wylosowanym zbiorze, przed każdym podziałem węzła losując bez zwracania podzbiór \begin{math}
            m \leq |D|
        \end{math} atrybutów uwzględnianych przy jego podziale.}
    \Else
        \State{Wylosuj bez zwracania podzbiór podzbiór \begin{math}
            m \leq |D|
        \end{math} atrybutów. Wytrenuj SVM na wylosowanym zbiorze, uwzględniając tylko wylosowane atrybuty.}
    \EndIf
    \State{Dodaj wytrenowany klasyfikator do zbioru klasyfikatorów lasu losowego.}
\end{algorithmic}
\end{algorithm}

\textit{Linia 1:}
Liczba wszystkich klasyfikatorów w lesie losowym jest hiperparametrem algorytmu lasu losowego, z zastrzeżeniem że musi być ona podzielna przez 2, ponieważ zakładamy że las składa się z drzew decyzyjnych oraz SVM na przemian.\\

\textit{Linia 2:}
Trenując każdy model w lesie losowym na różnych podzbiorach oryginalnego zbioru danych (z założeniem, że próbki danych w podzbiorze mogą się powtarzać) redukowana jest wariancja modelu, w skutek czego zmniejszane jest przeuczenie lasu.\\

\textit{Linie 4-6:}
Przed każdym podziałem węzła w drzewie decyzyjnym losowany jest bez zwracania podzbiór atrybutów rozważanych podczas podziału. Ta modyfikacja klasycznego algorytmu uczenia drzewa zmniejsza korelacje między poszczególnymi drzewami w lesie. Gdyby ta modyfikacja nie była zastosowana, atrybutami podziału węzłów w większości drzew w lesie byłyby takie, które najskuteczniej dzielą zbiór danych. Wskutek tego las składałby się ze skorelowanych drzew, co nie zwiększyłoby skuteczności modelu, ponieważ słabe klasyfikatory w lesie byłyby zgodne co do złych predykcji, a to by skutkowało błędnymi ostatecznymi predykcjami lasu. Podobnie przed uczeniem SVM losowany jest bez zwracania podzbiór atrybutów uwzględnianych w procesie uczenia - model uczy się tylko na tych atrybutach.
\subsection{Predykcje algorytmu}
Po wytrenowaniu wszystkich klasyfikatorów, predykcją lasu losowego jest klasa większościowa w zbiorze predykcji każdego z klasyfikatorów w drzewie.

\newpage
\section{Eksperymenty}
\subsection{Zbiory danych}
Eksperymenty zostały wykonane na 3 zbiorach danych opisanych w poniższej tabeli. Dla każdego ze zbiorów została wykonana walidacja krzyżowa, gdzie ostateczny wynik na zbiorze testowym był estymowany na zasadzie makro-uśredniania, tzn. jako średnia wartość wskaźników jakości uzyskanych na wszystkich podziałach.

% \begin{center}
\begin{table}[h]
\centering
\begin{tabular}{ |p{3cm}||p{6cm}|p{2cm}|p{2cm}|  }
    \hline
    \normalsize{Nazwa zbioru}& \normalsize{Opis} & \footnotesize{Liczba przykładów pozytywnych} & \footnotesize{Liczba przykładów negatywnych}\\
    \hline
     \textbf{Breast Cancer} \cite{datasetbreast} & Zbiór danych składający się z $569$ przykładów posiadających $30$ atrybutów ciągłych. & 212 & 357 \\
    \hline
     \textbf{Ionosphere}  \cite{datasetionosphere} & Zbiór danych składający się z $351$ przykładów posiadających $34$ atrybuty ciągłe. & $225$ & $126$\\
    \hline
     \textbf{QSAR biodegradation} \cite{datasetqsar} & Zbiór danych składający się z $1055$ przykładów posiadających $41$ atrybuty ciągłe. & $356$ &  $699$\\
    \hline
\end{tabular}
\caption{Opis zbiorów danych, które zostaną wykorzystane do eksperymentów numerycznych.}
\label{tab:my_label}
\end{table}
% \end{center}
\subsection{Analiza skuteczności hybrydy lasu losowego z SVM}
Wykonane zostały symulacje mające na celu porównanie zaimplementowanej hybrydy Lasu Losowego i SVM z klasycznym Lasem Losowym (z samymi drzewami decyzyjnymi), Lasem Losowym z samymi modelami SVM, modelem SVM i modelem Drzewa Decyzyjnego w zadaniu klasyfikacji binarnej pod względem metryk dokładności (accuracy), odzysku (recall) oraz precyzji (precision), a także metryki F1. Wnioski i analizy przeprowadzone zostały na zagregowanych wynikach z 25 uruchomień, na których wyliczona została średnia arytmetyczna, odchylenie standardowe czy wartości minimalne i maksymalne. Badania wykonane zostały na 3 zbiorach danych opisanych w poprzednim podpunkcie.
Hiperparametry dla modeli zostały dobra te, które dawały najlepsze rezultaty w kolejnym eksperymencie:
\begin{itemize}
    \item Liczba klasyfikatorów w lesie losowym: $14$.
    \item Maksymalna liczba atrybutów dla lasu losowego: $16$.
    \item Maksymalna głębokość drzewa: $6$.
    \item Minimalna różnica entropii: $0.01$.
    \item Minimalna liczba przykładów w węźle: $40$.
    \item Parametr $\lambda$ dla SVM: $0.005$.
    \item Dla SGD: parametr kroku $\beta=0.01$, $\epsilon_{min}=10^{-17}$ oraz  $n\_steps_{max}=10000$.
\end{itemize}

\subsubsection{Wyniki na zbiorach: Breast Cancer, Ionosphere , Biodegradation }
\begin{center}
\begin{table}[h]
\small
\centering
\begin{tabular}{ |p{5cm}||p{2cm}|p{2cm}|p{2cm}|p{2cm}|  }
    \hline
     \footnotesize{Model}&  \footnotesize{Accuracy} & \footnotesize{Recall} & \footnotesize{Precision} & \footnotesize{F1}\\
    \hline
     \textbf{Model SVM} &  \makecell{0.88 \pm 0.05 \\ min:0.75 \\ max:0.92}  & \makecell{0.91 \pm 0.07 \\ min:0.66 \\ \textbf{max:0.98}} & \makecell{0.92 \pm 0.02 \\ min:0.87 \\ max:0.95} & \makecell{0.90 \pm 0.04 \\ min:0.76 \\ max:0.94} \\
    \hline
     \textbf{Drzewo decyzyjne} &  \makecell{0.90 \pm 0.01 \\ min:0.88 \\ max:0.91}  & \makecell{0.94 \pm 0.02 \\ min:0.91 \\ max:0.97} & \makecell{0.91 \pm 0.01 \\ min:0.88 \\ max:0.93} & \makecell{0.92 \pm 0.01 \\ min:0.91 \\ max:0.93} \\
    \hline
     \textbf{Las losowy - hybryda}  &  \makecell{\textbf{0.93} \pm 0.01 \\ min:\textbf{0.91} \\ max:\textbf{0.94}}  & \makecell{\textbf{0.95} \pm 0.01 \\ min:\textbf{0.93} \\ max:\textbf{0.98}} & \makecell{\textbf{0.93} \pm 0.01 \\ min:0.90 \\ max:\textbf{0.95}} & \makecell{\textbf{0.94} \pm 0.01 \\ min:\textbf{0.93} \\ max:\textbf{0.96}} \\
    \hline
     \textbf{Las losowy - same SVM} &  \makecell{0.90 \pm 0.02 \\ min:0.85 \\ max:0.93}  & \makecell{0.92 \pm 0.03 \\ min:0.83 \\ max:\textbf{0.98}} & \makecell{0.92 \pm 0.01 \\ min:\textbf{0.91} \\ max:0.94} & \makecell{0.92 \pm 0.01 \\ min:0.87 \\ max:0.95} \\
    \hline
     \textbf{Las losowy - klasyczny} &  \makecell{0.92 \pm 0.01 \\ min:0.90 \\ max:\textbf{0.94}}  & \makecell{\textbf{0.95} \pm 0.01 \\ min:\textbf{0.93} \\ max:0.97} & \makecell{\textbf{0.93} \pm 0.01 \\ min:\textbf{0.91} \\ max:0.94} & \makecell{\textbf{0.94} \pm 0.01 \\ min:0.92 \\ max:0.95} \\
    \hline
\end{tabular}
\caption{Porównanie działania różnych typów modeli pod względem metryk accuracy, recall, precision i f1 na zbiorze Breast Cancer \cite{datasetbreast}.}
\label{tab:my_label}
\end{table}
\end{center}

\begin{center}
\begin{table}[!htbp]
\small
\centering
\begin{tabular}{ |p{5cm}||p{2cm}|p{2cm}|p{2cm}|p{2cm}|  }
    \hline
     \footnotesize{Model}&  \footnotesize{Accuracy} & \footnotesize{Recall} & \footnotesize{Precision} & \footnotesize{F1}\\
    \hline
    \textbf{Model SVM} & \makecell{0.83 \pm 0.01 \\ min:0.8 \\ max:0.86} & \makecell{0.93 \pm 0.02 \\ min:0.89 \\ max:0.90} & \makecell{0.83 \pm 0.01 \\ min:0.81 \\ max:0.85} & \makecell{0.88 \pm 0.01 \\ min:0.85 \\ max:0.9} \\
    \hline
    \textbf{Drzewo decyzyjne} & \makecell{\textbf{0.87} \pm 0.02 \\ min:0.81 \\ max:\textbf{0.9}} & \makecell{0.95 \pm 0.01 \\ min:0.93 \\ max:0.98} & \makecell{\textbf{0.87} \pm 0.03 \\ min:0.8 \\ max:\textbf{0.92}} & \makecell{\textbf{0.9} \pm 0.01 \\ min:\textbf{0.88} \\ max:\textbf{0.93}} \\
    \hline
    \textbf{Las losowy - hybryda} & \makecell{0.86 \pm 0.01 \\ min:\textbf{0.84} \\ max:0.89} & \makecell{\textbf{0.98} \pm 0.01 \\ min:\textbf{0.96} \\ max:0.99} & \makecell{0.84 \pm 0.01 \\ min:0.81 \\ max:0.86} & \makecell{\textbf{0.9} \pm 0.01 \\ min:\textbf{0.88} \\ max:0.92} \\
    \hline
    \textbf{Las losowy - same SVM} & \makecell{0.85 \pm 0.01 \\ min:0.83 \\ max:0.87} & \makecell{0.95 \pm 0.01 \\ min:0.93 \\ max:0.96} & \makecell{0.84 \pm 0.01 \\ min:\textbf{0.82} \\ max:0.85} & \makecell{0.89 \pm 0.01 \\ min:\textbf{0.88} \\ max:0.9} \\
    \hline
    \textbf{Las losowy - klasyczny} & \makecell{0.85 \pm 0.02 \\ min:0.82 \\ max:0.88} & \makecell{\textbf{0.98} \pm 0.01 \\ min:\textbf{0.96} \\ max:\textbf{1.0}} & \makecell{0.82 \pm 0.02 \\ min:0.79 \\ max:0.85} & \makecell{0.89 \pm 0.01 \\ min:0.87 \\ max:0.91} \\
    \hline
\end{tabular}
\caption{Porównanie działania różnych typów modeli pod względem metryk accuracy, recall, precision i f1 na zbiorze Ionosphere \cite{datasetionosphere}.} 
\label{tab:my_label}
\end{table}
\end{center}

\begin{center}
\begin{table}[h]
\small
\centering
\begin{tabular}{ |p{5cm}||p{2cm}|p{2cm}|p{2cm}|p{2cm}|  }
    \hline
     \footnotesize{Model}&  \footnotesize{Accuracy} & \footnotesize{Recall} & \footnotesize{Precision} & \footnotesize{F1}\\
    \hline
    \textbf{Model SVM} & \makecell{0.76 \pm 0.02 \\ min:0.73 \\ max:0.79} & \makecell{\textbf{0.69} \pm 0.15 \\ min:0.41 \\ max:\textbf{0.95}} & \makecell{0.73 \pm 0.07 \\ min:0.62 \\ max:\textbf{0.86}} & \makecell{0.63 \pm 0.08 \\ min:0.47 \\ max:0.74} \\
    \hline
    \textbf{Drzewo decyzyjne} & \makecell{0.78 \pm 0.01 \\ min:0.76 \\ max:0.81} & \makecell{0.67 \pm 0.03 \\ min:\textbf{0.61} \\ max:0.71} & \makecell{0.69 \pm 0.02 \\ min:0.67 \\ max:0.74} & \makecell{0.67 \pm 0.02 \\ min:0.64 \\ max:0.72} \\
    \hline
    \textbf{Las losowy - hybryda} & \makecell{\textbf{0.82} \pm 0.01 \\ min:\textbf{0.81} \\ max:\textbf{0.84}} & \makecell{0.65 \pm 0.05 \\ min:0.55 \\ max:0.73} & \makecell{\textbf{0.79} \pm 0.02 \\ min:\textbf{0.74} \\ max:0.85} & \makecell{\textbf{0.71} \pm 0.03 \\ min:0.65 \\ max:\textbf{0.75}} \\
    \hline
    \textbf{Las losowy - same SVM} & \makecell{0.81 \pm 0.01 \\ min:0.78 \\ max:\textbf{0.84}} & \makecell{0.64 \pm 0.07 \\ min:0.44 \\ max:0.75} & \makecell{0.78 \pm 0.02 \\ min:0.72 \\ max:0.81} & \makecell{0.67 \pm 0.04 \\ min:0.57 \\ max:\textbf{0.75}} \\
    \hline
    \textbf{Las losowy - klasyczny} & \makecell{0.81 \pm 0.01 \\ min:0.8 \\ max:0.82} & \makecell{0.65 \pm 0.03 \\ min:0.58 \\ max:0.73} & \makecell{0.75 \pm 0.02 \\ min:0.73 \\ max:0.79} & \makecell{0.69 \pm 0.02 \\ min:\textbf{0.66} \\ max:0.74} \\ 
    \hline
\end{tabular}
\caption{Porównanie wyników różnych modeli na zbiorze Biodegradation \cite{datasetqsar}.}
\label{tab:my_label}
\end{table}
\end{center}

\subsubsection{Wnioski}
Na podstawie wyników przeprowadzonych eksperymentów można zauważyć, że zaimplementowana hybryda lasu losowego z Drzewami Decyzyjnymi i SVM w większości przypadków osiąga najlepsze wyniki względem pozostałych modeli, które brały udział w badaniu. W przypadku zbiorów Breast Cancer i Biodegradation, hybrydowy Las Losowy osiąga najlepsze wyniki pod względem metryk Accuracy oraz F1, które są najbardziej pożądanymi metrykami. W przypadku zbioru Ionosphere możemy natomiast zauważyć brak dominacji hybrydy względem pozostałych modeli na tle tych dwóch metryk - spowodowane to może być mniejszym skomplikowaniem domeny tego zbioru - w takim przypadku bardziej pożądane mogą okazać się modele prostsze, takie jak model SVM czy Drzewo Decyzyjne, których trening trwa zdecydowanie krócej, a interferencja osiąga bardzo podobne wyniki. Istotnym jest również zwrócenie uwagi na stabilność w wynikach modelu - zauważyć można, że, spośród wszystkich badanych modeli, wyniki hybrydy osiągają największą stabilność (posiadają najmniejsze odchylenie standardowe na wszystkich zbiorach w niemal wszystkich metrykach) - co jest dużą zaletą tego rozwiązania. Ciekawe wyniki można zauważyć w przypadku zbioru Biodegradation - tylko dla niego ma miejsce sytuacja, gdzie model hybrydowy wypada najgorzej pod względem metryki Recall i najlepiej pod względem metryki Precision na tle pozostałych modeli, co może być związane z faktem, że ten zbiór jest najmniej zbilansowanym w porównaniu do pozostałych - stąd też tutaj bardziej miarodajna wydaje się metryka F1, dla której hybryda osiąga najlepsze wyniki. Na podstawie tych wniosków, można stwierdzić, że wykonana implementacja osiągnęła bardzo korzystne wyniki. 

\subsection{Analiza wpływu hiperparametrów na skuteczność hybrydy}
W ramach kolejnego eksperymentu zbadano wpływ hiperparametrów na skuteczność zaimplementowanej hybrydy lasu losowego z SVM.
Wszystkie eksperymenty przeprowadzone zostały dla 3 zbiorów danych wymienionych wcześniej. Dla każdego hiperparametru dobrana została lista badanych jego wartości i dla każdej z tych wartości wyliczono wartość metryki accuracy, precision, recall i f1-score jako średnia arytmetyczna, odchylenie standardowe, wartości minimalne i maksymalne na podstawie wyników zagregowanych z 25 uruchomień. W każdym eksperymencie została zastosowana 5-krotna walidacja krzyżowa, a ostateczny wynik metryki dla każdego uruchomienia to średnia wartość metryki dla 5 podzbiorów testowych.
Gdy dane hiperparametry nie są badane w eksperymencie, przyjmują one wartości odpowiednio:
\begin{itemize}
    \item Liczba klasyfikatorów w lesie losowym: $14$.
    \item Maksymalna liczba atrybutów dla lasu losowego: $16$.
    \item Maksymalna głębokość drzewa: $6$.
    \item Minimalna różnica entropii: $0.01$.
    \item Minimalna liczba przykładów w węźle: $40$.
    \item Parametr $\lambda$ dla SVM: $0.005$.
    \item Dla SGD: parametr kroku $\beta=0.01$, $\epsilon_{min}=10^{-17}$ oraz $n\_steps_{max}$ $=10000$.
\end{itemize}


\subsubsection{Wpływ liczby klasyfikatorów}
Na podstawie uzyskanych wyników w tabeli \ref{tab:clf_table} widać wyraźnie, że algorytm osiąga słabe wyniki, kiedy liczba klasyfikatorów jest bardzo mała. Nie jest to zaskakujące, pojedyncze klasyfikatory w lesie są słabymi modelami, a gdy jest ich za mało to nawet uśrednienie predykcji lasu losowego przez przedstawienie ostatecznej predykcji jako klasy większościowej nic nie daje. Dla małej liczby klasyfikatorów zauważalne są też duże wartości odchylenia standardowego. Wzrost ten jest jednak ograniczony. Można zauważyć, że nie ma już bardzo znaczącej różnicy między skutecznością algorytmu złożonego z 14, a złożonego z 20 klasyfikatorów. Również przy większej liczbie klasyfikatorów odchylenia standardowe bardzo maleją, co świadczy o stabilności i dobrej generalizacji modelu. Ciekawą obserwacją jest również to, że głównie na poprawę skuteczności algorytmu wraz ze wzrostem liczby klasyfikatorów wpływa poprawa metryki recall. Metryka precision za to zmienia się w bardzo małym stopniu. Znaczne zwiększenie liczby klasyfikatorów w drzewie do liczby 200 zwiększa skuteczność modelu, chociaż nie zmienia się ona diametralnie. Da się jednak zauważyć na podstawie wyników w tabeli, że większość metryk zwiększa swoje wartości. Wniosek z tego taki, że model, gdy liczba klasyfikatorów w drzewie jest większa potrafi lepiej generalizować, a więc osiąga lepsze wyniki na zbiorze testowym.
% \begin{center}
\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{ |p{2cm}||p{3cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|  }
    \hline
    \footnotesize{Liczba klasyfikatorów} & \footnotesize{Nazwa zbioru}&  \footnotesize{Accuracy} & \footnotesize{Recall} & \footnotesize{Precision} & \footnotesize{F1}\\
    \hline
     \textbf{2} & Breast Cancer \cite{datasetbreast} &  \makecell{0.79 \pm 0.08 \\ min:0.59 \\ max:0.85}  & \makecell{0.71 \pm 0.14 \\ min:0.52 \\ max:0.91} & \makecell{0.92 \pm 0.08 \\ min:0.73 \\ max:0.98} & \makecell{0.76 \pm 0.12 \\ min:0.40 \\ max:0.93} \\
    \hline
     \textbf{8} & Breast Cancer \cite{datasetbreast} &  \makecell{0.92 \pm 0.17 \\ min:0.85 \\ max:0.93}  & \makecell{0.93 \pm 0.03 \\ min:0.82 \\ max:0.97} & \makecell{\textbf{0.93} \pm 0.02 \\ min:0.91 \\ max:0.96} & \makecell{0.93 \pm 0.02 \\ min:0.85 \\ max:0.95} \\
    \hline
     \textbf{14} & Breast Cancer \cite{datasetbreast} &  \makecell{0.92 \pm 0 \\ min:0.90 \\ max:0.93}  & \makecell{0.95 \pm 0.02 \\ min:0.92 \\ max:0.97} & \makecell{\textbf{0.93} \pm 0.01 \\ min:0.90 \\ max:0.96} & \makecell{0.94 \pm 0 \\ min:0.93 \\ max:0.95} \\
     \hline
     \textbf{20} & Breast Cancer \cite{datasetbreast} &  \makecell{0.92 \pm 0 \\ min:0.91 \\ max:0.93}  & \makecell{0.95 \pm 0.01 \\ min:0.94 \\ max:0.97} & \makecell{\textbf{0.93} \pm 0.01 \\ min:0.90 \\ max:0.95} & \makecell{0.94 \pm 0 \\ min:0.93 \\ max:0.95} \\
     \hline
     \textbf{200} & Breast Cancer \cite{datasetbreast} &  \makecell{\textbf{0.93} \pm 0.01 \\ min:0.92 \\ max:0.94}  & \makecell{\textbf{0.97} \pm 0 \\ min:0.96 \\ max:0.97} & \makecell{0.92 \pm 0.01 \\ min:0.91 \\ max:0.93} & \makecell{\textbf{0.95} \pm 0.01 \\ min:0.93 \\ max:0.95} \\
     \hline
     \textbf{2} & Ionosphere \cite{datasetionosphere} &  \makecell{0.84 \pm 0.02 \\ min:0.81 \\ max:0.87}  & \makecell{0.90 \pm 0.03 \\ min:0.85 \\ max:0.95} & \makecell{\textbf{0.87} \pm 0.02 \\ min:0.83 \\ max:0.90} & \makecell{0.88 \pm 0.01 \\ min:0.86 \\ max:0.91} \\
    \hline
     \textbf{8} & Ionosphere \cite{datasetionosphere} &  \makecell{\textbf{0.87} \pm 0.01 \\ min:0.84 \\ max:0.89}  & \makecell{0.98 \pm 0 \\ min:0.95 \\ max:0.99} & \makecell{0.84 \pm 0.01 \\ min:0.82 \\ max:0.85} & \makecell{\textbf{0.90} \pm 0 \\ min:0.88 \\ max:0.92} \\
    \hline
     \textbf{14} & Ionosphere \cite{datasetionosphere} &  \makecell{\textbf{0.87} \pm 0 \\ min:0.85 \\ max:0.88}  & \makecell{0.98 \pm 0 \\ min:0.97 \\ max:0.98} & \makecell{0.84 \pm 0 \\ min:0.83 \\ max:0.84} & \makecell{\textbf{0.90} \pm 0 \\ min:0.89 \\ max:0.91} \\
     \hline
     \textbf{20} & Ionosphere \cite{datasetionosphere} &  \makecell{0.86 \pm 0.01 \\ min:0.85 \\ max:0.89}  & \makecell{0.98 \pm 0 \\ min:0.98 \\ max:0.99} & \makecell{0.84 \pm 0.01 \\ min:0.82 \\ max:0.84} & \makecell{\textbf{0.90} \pm 0 \\ min:0.91 \\ max:0.92} \\
     \hline
     \textbf{200} & Ionosphere \cite{datasetionosphere} &  \makecell{0.86 \pm 0.01 \\ min:0.85 \\ max:0.87}  & \makecell{\textbf{0.99} \pm 0 \\ min:0.98 \\ max:1.0} & \makecell{0.83 \pm 0.01 \\ min:0.81 \\ max:0.84} & \makecell{\textbf{0.90} \pm 0 \\ min:0.89 \\ max:0.91} \\
     \hline
     \textbf{2} & QSAR biodegradation \cite{datasetqsar} &  \makecell{0.74 \pm 0.02 \\ min:0.71 \\ max:0.76}  & \makecell{0.35 \pm 0.11 \\ min:0.14 \\ max:0.53} & \makecell{0.69 \pm 0.13 \\ min:0.41 \\ max:0.83} & \makecell{0.41 \pm 0.11 \\ min:0.25 \\ max:0.62} \\
    \hline
     \textbf{8} & QSAR biodegradation \cite{datasetqsar} &  \makecell{0.80 \pm 0.01 \\ min:0.78 \\ max:0.81}  & \makecell{0.59 \pm 0.06 \\ min:0.45 \\ max:0.68} & \makecell{\textbf{0.79} \pm 0.03 \\ min:0.76 \\ max:0.82} & \makecell{0.66 \pm 0.04 \\ min:0.60 \\ max:0.72} \\
    \hline
     \textbf{14} & QSAR biodegradation \cite{datasetqsar} &  \makecell{0.81 \pm 0.01 \\ min:0.80 \\ max:0.83}  & \makecell{0.62 \pm 0.07 \\ min:0.50 \\ max:0.72} & \makecell{\textbf{0.79} \pm 0.04 \\ min:0.75 \\ max:0.85} & \makecell{0.68 \pm 0.04 \\ min:0.67 \\ max:0.72} \\
     \hline
     \textbf{20} & QSAR biodegradation \cite{datasetqsar} &  \makecell{0.82 \pm 0 \\ min:0.80 \\ max:0.83}  & \makecell{0.65 \pm 0.04 \\ min:0.57 \\ max:0.67} & \makecell{0.78 \pm 0.02 \\ min:0.77 \\ max:0.83} & \makecell{0.70 \pm 0.03 \\ min:0.67 \\ max:0.73} \\
    \hline
     \textbf{200} & QSAR biodegradation \cite{datasetqsar} &  \makecell{\textbf{0.83} \pm 0.01 \\ min:0.82 \\ max:0.84}  & \makecell{\textbf{0.69} \pm 0.01 \\ min:0.68 \\ max:0.7} & \makecell{\textbf{0.79} \pm 0.01 \\ min:0.77 \\ max:0.81} & \makecell{\textbf{0.74} \pm 0.01 \\ min:0.73 \\ max:0.75} \\
    \hline
\end{tabular}
\caption{Analiza wpływu liczby klasyfikatorów na działanie algorytmu.}
\label{tab:clf_table}
\end{table}
% \end{center}


\newpage
\subsubsection{Wpływ maksymalnej głębokości drzewa}
% \begin{center}
\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{ |p{2cm}||p{3cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|  }
    \hline
    \footnotesize{Maksymalna głębokość drzewa} & \footnotesize{Nazwa zbioru}&  \footnotesize{Accuracy} & \footnotesize{Recall} & \footnotesize{Precision} & \footnotesize{F1}\\
    \hline
     \textbf{2} & Breast Cancer \cite{datasetbreast} &  \makecell{\textbf{0.92} \pm 0 \\ min:0.91 \\ max:0.93}  & \makecell{\textbf{0.95} \pm 0.01 \\ min:0.94 \\ max:0.98} & \makecell{\textbf{0.93} \pm 0.02 \\ min:0.90 \\ max:0.94} & \makecell{\textbf{0.94} \pm 0 \\ min:0.93 \\ max:0.95} \\
    \hline
     \textbf{6} & Breast Cancer \cite{datasetbreast} &  \makecell{\textbf{0.92} \pm 0 \\ min:0.91 \\ max:0.93}  & \makecell{0.94 \pm 0.01 \\ min:0.93 \\ max:0.96} & \makecell{\textbf{0.93} \pm 0.02 \\ min:0.90 \\ max:0.95} & \makecell{\textbf{0.94} \pm 0 \\ min:0.93 \\ max:0.95} \\
    \hline
     \textbf{10} & Breast Cancer \cite{datasetbreast} &  \makecell{\textbf{0.92} \pm 0.01 \\ min:0.89 \\ max:0.94}  & \makecell{\textbf{0.95} \pm 0.02 \\ min:0.93 \\ max:0.97} & \makecell{\textbf{0.93} \pm 0.01 \\ min:0.92 \\ max:0.95} & \makecell{\textbf{0.94} \pm 0.01 \\ min:0.93 \\ max:0.96} \\
     \hline
     \textbf{2} & Ionosphere \cite{datasetionosphere} &  \makecell{\textbf{0.86} \pm 0.01 \\ min:0.84 \\ max:0.87}  & \makecell{\textbf{0.98} \pm 0 \\ min:0.96 \\ max:0.99} & \makecell{\textbf{0.84} \pm 0.01\\ min:0.82 \\ max:0.85} & \makecell{\textbf{0.90} \pm 0 \\ min:0.89 \\ max:0.91} \\
    \hline
     \textbf{6} & Ionosphere \cite{datasetionosphere} &  \makecell{\textbf{0.86} \pm 0 \\ min:0.85 \\ max:0.87}  & \makecell{\textbf{0.98} \pm 0 \\ min:0.95 \\ max:0.99} & \makecell{0.83 \pm 0.01 \\ min:0.82 \\ max:0.84} & \makecell{\textbf{0.90} \pm 0 \\ min:0.88 \\ max:0.92} \\
    \hline
     \textbf{10} & Ionosphere \cite{datasetionosphere} &  \makecell{\textbf{0.86} \pm 0.01 \\ min:0.83 \\ max:0.88}  & \makecell{\textbf{0.98} \pm 0.01 \\ min:0.97 \\ max:0.99} & \makecell{0.83 \pm 0.02 \\ min:0.80 \\ max:0.86} & \makecell{\textbf{0.90} \pm 0.01 \\ min:0.88 \\ max:0.91} \\
     \hline
     \textbf{2} & QSAR biodegradation \cite{datasetqsar} &  \makecell{0.79 \pm 0.01 \\ min:0.78 \\ max:0.81}  & \makecell{0.59 \pm 0.05 \\ min:0.49 \\ max:0.61} & \makecell{0.77 \pm 0.03 \\ min:0.70 \\ max:0.81} & \makecell{0.65 \pm 0.03 \\ min:0.58 \\ max:0.69} \\
     \hline
     \textbf{6} & QSAR biodegradation \cite{datasetqsar} &  \makecell{\textbf{0.81} \pm 0.01 \\ min:0.80 \\ max:0.83}  & \makecell{\textbf{0.65} \pm 0.04 \\ min:0.59 \\ max:0.66} & \makecell{\textbf{0.78} \pm 0.03 \\ min:0.75 \\ max:0.83} & \makecell{\textbf{0.70} \pm 0.02 \\ min:0.67 \\ max:0.73} \\
    \hline
     \textbf{10} & QSAR biodegradation \cite{datasetqsar} &  \makecell{\textbf{0.81} \pm 0.01 \\ min:0.77 \\ max:0.83}  & \makecell{0.63 \pm 0.06 \\ min:0.48 \\ max:0.72} & \makecell{\textbf{0.78} \pm 0.02 \\ min:0.78 \\ max:0.82} & \makecell{0.69 \pm 0.04 \\ min:0.57 \\ max:0.72} \\
    \hline
\end{tabular}
\caption{Analiza wpływu maksymalnej głębokości drzewa decyzyjnego na działanie algorytmu.}
\label{tab:dpth_table}
\end{table}
% \end{center}

Wyniki dla wszystkich zmierzonych wartości maksymalnej głębokości drzewa są bardzo zbliżone, co widać w tabeli \ref{tab:dpth_table}. Ciekawym spostrzeżeniem jest większa wartość odchylenia standardowego dla większej maksymalnej głębokości w większości przypadków. Może to być oznaką lekkiego przeuczania się modelu - drzewa stają się zbyt rozbudowane przez co za bardzo dopasowują się do zbioru danych. Niemniej jednak, jako że otrzymano podobne wyniki w każdym eksperymencie, można wysnuć wniosek, że zmiana wartości hiperparametrów tylko jednego z dwóch użytych klasyfikatorów w naszym algorytmie nie będzie drastycznie wpływała na skuteczność całego modelu hybrydowego.

\subsubsection{Wpływ parametru $\lambda$ w SVM}
% \begin{center}
\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{ |p{2cm}||p{3cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|  }
    \hline
    \footnotesize{$\lambda$} & \footnotesize{Nazwa zbioru}&  \footnotesize{Accuracy} & \footnotesize{Recall} & \footnotesize{Precision} & \footnotesize{F1}\\
    \hline
     \textbf{0.005} & Breast Cancer \cite{datasetbreast} &  \makecell{\textbf{0.93} \pm 0 \\ min:0.92 \\ max:0.94}  & \makecell{\textbf{0.95} \pm 0.01 \\ min:0.92 \\ max:0.98} & \makecell{\textbf{0.93} \pm 0.01 \\ min:0.92 \\ max:0.95} & \makecell{\textbf{0.94} \pm 0 \\ min:0.93 \\ max:0.94} \\
    \hline
     \textbf{0.05} & Breast Cancer \cite{datasetbreast} &  \makecell{0.92 \pm 0 \\ min:0.91 \\ max:0.93}  & \makecell{\textbf{0.95} \pm 0.01 \\ min:0.93 \\ max:0.96} & \makecell{\textbf{0.93} \pm 0.01 \\ min:0.92 \\ max:0.96} & \makecell{\textbf{0.94} \pm 0 \\ min:0.93 \\ max:0.95} \\
    \hline
     \textbf{0.5} & Breast Cancer \cite{datasetbreast} &  \makecell{0.91 \pm 0.02 \\ min:0.89 \\ max:0.94}  & \makecell{0.93 \pm 0.03 \\ min:0.90 \\ max:0.97} & \makecell{\textbf{0.93} \pm 0.02 \\ min:0.91 \\ max:0.94} & \makecell{0.93 \pm 0.02 \\ min:0.92 \\ max:0.94} \\
     \hline
     \textbf{5} & Breast Cancer \cite{datasetbreast} &  \makecell{0.92 \pm 0 \\ min:0.89 \\ max:0.93}  & \makecell{0.94 \pm 0.02 \\ min:0.88 \\ max:0.97} & \makecell{\textbf{0.93} \pm 0.02 \\ min:0.91 \\ max:0.95} & \makecell{0.93 \pm 0 \\ min:0.91 \\ max:0.94} \\
     \hline
     \textbf{0.005} & Ionosphere \cite{datasetionosphere} &  \makecell{\textbf{0.87} \pm 0.01 \\ min:0.85 \\ max:0.88}  & \makecell{0.98 \pm 0 \\ min:0.97 \\ max:0.98} & \makecell{\textbf{0.84} \pm 0.01 \\ min:0.82 \\ max:0.86} & \makecell{\textbf{0.91} \pm 0 \\ min:0.90 \\ max:0.93} \\
    \hline
     \textbf{0.05} & Ionosphere \cite{datasetionosphere} &  \makecell{0.86 \pm 0.01 \\ min:0.86 \\ max:0.87}  & \makecell{0.98 \pm 0 \\ min:0.96 \\ max:0.99} & \makecell{0.83 \pm 0.01 \\ min:0.82 \\ max:0.85} & \makecell{0.90 \pm 0 \\ min:0.89 \\ max:0.91} \\
    \hline
     \textbf{0.5} & Ionosphere \cite{datasetionosphere} &  \makecell{0.86 \pm 0.01 \\ min:0.85 \\ max:0.88}  & \makecell{0.98 \pm 0 \\ min:0.97 \\ max:0.98} & \makecell{0.83 \pm 0.01 \\ min:0.83 \\ max:0.84} & \makecell{0.90 \pm 0 \\ min:0.89 \\ max:0.91} \\
     \hline
     \textbf{5} & Ionosphere \cite{datasetionosphere} &  \makecell{0.84 \pm 0.01 \\ min:0.83 \\ max:0.86}  & \makecell{\textbf{0.99} \pm 0 \\ min:0.98 \\ max:0.99} & \makecell{0.81 \pm 0.01 \\ min:0.79 \\ max:0.82} & \makecell{0.89 \pm 0 \\ min:0.88 \\ max:0.90} \\
     \hline
     \textbf{0.005} & QSAR biodegradation \cite{datasetqsar} &  \makecell{\textbf{0.82} \pm 0 \\ min:0.80 \\ max:0.83}  & \makecell{\textbf{0.66} \pm 0.11 \\ min:0.57 \\ max:0.71} & \makecell{0.79 \pm 0.02 \\ min:0.78 \\ max:0.81} & \makecell{\textbf{0.71} \pm 0.03 \\ min:0.67 \\ max:0.73} \\
    \hline
     \textbf{0.05} & QSAR biodegradation \cite{datasetqsar} &  \makecell{0.81 \pm 0 \\ min:0.80 \\ max:0.82}  & \makecell{0.63 \pm 0.05 \\ min:0.53 \\ max:0.73} & \makecell{0.78 \pm 0.02 \\ min:0.76 \\ max:0.83} & \makecell{0.69 \pm 0.02 \\ min:0.67 \\ max:0.72} \\
    \hline
     \textbf{0.5} & QSAR biodegradation \cite{datasetqsar} &  \makecell{0.79 \pm 0.01 \\ min:0.77 \\ max:0.82}  & \makecell{0.53 \pm 0.05 \\ min:0.44 \\ max:0.57} & \makecell{\textbf{0.81} \pm 0.03 \\ min:0.78 \\ max:0.84} & \makecell{0.62 \pm 0.04 \\ min:0.58 \\ max:0.67} \\
     \hline
     \textbf{5} & QSAR biodegradation \cite{datasetqsar} &  \makecell{0.76 \pm 0.02 \\ min:0.74 \\ max:0.79}  & \makecell{0.36 \pm 0.08 \\ min:0.18 \\ max:0.46} & \makecell{0.80 \pm 0.10 \\ min:0.61 \\ max:0.92} & \makecell{0.47 \pm 0.08 \\ min:0.42 \\ max:0.61} \\
    \hline
\end{tabular}
\caption{Analiza wpływu parametru $\lambda$ w SVM na działanie algorytmu.}
\label{tab:lambda_table}
\end{table}
% \end{center}

Na podstawie analizy wyników z tego eksperymentu, przedstawionych w tabeli \ref{tab:lambda_table}, dojść można do ciekawych obserwacji, które były już zauważalne podczas analizy wpływu maksymalnej głębokości drzewa decyzyjnego na skuteczność hybrydy, niemniej jednak w dużo mniejszym stopniu. Zmiana hiperparametru pojedynczego typu klasyfikatora w hybrydzie dużo bardziej oddziałuje na skuteczność algorytmu na trudniejszym zbiorze danych, niż na względnie łatwym. Podobnie jak to było we wspomnianym wcześniej eksperymencie, wyniki dla każdej możliwej wartości $\lambda$ dla dwóch pierwszych badanych zbiorów danych są bardzo podobne, po czym można by wysnuć wniosek, że parametr ten nie wpływa znacząco na skuteczność hybrydy. Jednakże, po analizie wyników dla ostatniego zbioru danych widać, że parametr ten wpływa i to w bardzo znaczącym stopniu. Najbardziej widoczna jest różnica w metryce F1, gdzie w najlepszym przypadku jest ona równa $0.71$, a w najgorszym $0.47$. Znowu jak to było we wcześniejszych eksperymentach, tak i tu zmienia się głównie wartość metryki recall. Z eksperymentów można wyciągnąć wniosek, że model działa dużo lepiej dla małego współczynnika regularyzacji w klasyfikatorze SVM. Może być tak z tego powodu, że współczynnik ten spełnia bardzo istotną rolę zapobiegając przeuczeniu, gdy SVM uczony jest klasycznie tj. na całym zbiorze danych i wszystkich atrybutach. W tym przypadku pamiętać trzeba, że pojedynczy klasyfikator uczony jest tylko na podzbiorze atrybutów i danych, co już zapobiega nadmiernemu dopasowywaniu się modeli. Dlatego też nie jest konieczny wysoki współczynnik $\lambda$.

\subsubsection{Wpływ maksymalnej liczby atrybutów branej pod uwagę przy uczeniu klasyfikatorów}


% \begin{center}
\begin{table}[H]
\centering
\footnotesize
\begin{tabular}{ |p{2cm}||p{3cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|  }
    \hline
    \footnotesize{Maksymalna liczba atrybutów} & \footnotesize{Nazwa zbioru}&  \footnotesize{Accuracy} & \footnotesize{Recall} & \footnotesize{Precision} & \footnotesize{F1}\\
    \hline
     \textbf{8} & Breast Cancer \cite{datasetbreast} &  \makecell{0.92 \pm 0 \\ min:0.90 \\ max:0.93}  & \makecell{0.94 \pm 0.02 \\ min:0.94 \\ max:0.98} & \makecell{\textbf{0.93} \pm 0.01 \\ min:0.90 \\ max:0.95} & \makecell{\textbf{0.94} \pm 0 \\ min:0.92 \\ max:0.95} \\
    \hline
     \textbf{16} & Breast Cancer \cite{datasetbreast} &  \makecell{0.92 \pm 0 \\ min:0.90 \\ max:0.93}  & \makecell{0.95 \pm 0.02 \\ min:0.92 \\ max:0.96} & \makecell{\textbf{0.93} \pm 0.01 \\ min:0.91 \\ max:0.94} & \makecell{0.93 \pm 0 \\ min:0.92 \\ max:0.95} \\
    \hline
     \textbf{24} & Breast Cancer \cite{datasetbreast} &  \makecell{\textbf{0.93} \pm 0 \\ min:0.92 \\ max:0.94}  & \makecell{\textbf{0.96} \pm 0.01 \\ min:0.92 \\ max:0.97} & \makecell{\textbf{0.93} \pm 0.01 \\ min:0.91 \\ max:0.93} & \makecell{\textbf{0.94} \pm 0 \\ min:0.92 \\ max:0.94} \\
     \hline
     \textbf{8} & Ionosphere \cite{datasetionosphere} &  \makecell{0.83 \pm 0.01 \\ min:0.83 \\ max:0.96}  & \makecell{\textbf{0.99} \pm 0 \\ min:0.97 \\ max:0.99} & \makecell{0.80 \pm 0.01 \\ min:0.79 \\ max:0.83} & \makecell{0.88 \pm 0 \\ min:0.87 \\ max:0.90} \\
     \hline
     \textbf{16} & Ionosphere \cite{datasetionosphere} &  \makecell{0.86 \pm 0.01 \\ min:0.85 \\ max:0.89}  & \makecell{0.98 \pm 0 \\ min:0.97 \\ max:0.99} & \makecell{0.83 \pm 0.01 \\ min:0.82 \\ max:0.86} & \makecell{0.90 \pm 0 \\ min:0.90 \\ max:0.92} \\
    \hline
     \textbf{24} & Ionosphere \cite{datasetionosphere} &  \makecell{\textbf{0.87} \pm 0 \\ min:0.85 \\ max:0.89}  & \makecell{0.98 \pm 0 \\ min:0.97 \\ max:0.99} & \makecell{\textbf{0.85} \pm 0 \\ min:0.83 \\ max:0.87} & \makecell{\textbf{0.91} \pm 0 \\ min:0.90 \\ max:0.91} \\
    \hline
     \textbf{8} & QSAR biodegradation \cite{datasetqsar} &  \makecell{0.80 \pm 0.01 \\ min:0.79 \\ max:0.82}  & \makecell{0.58 \pm 0.06 \\ min:0.42 \\ max:0.63} & \makecell{0.79 \pm 0.02 \\ min:0.76 \\ max:0.84} & \makecell{0.65 \pm 0.04 \\ min:0.52 \\ max:0.69} \\
     \hline
     \textbf{16} & QSAR biodegradation \cite{datasetqsar} &  \makecell{\textbf{0.81} \pm 0.01 \\ min:0.78 \\ max:0.83}  & \makecell{0.59 \pm 0.06 \\ min:0.48 \\ max:0.70} & \makecell{\textbf{0.80} \pm 0.03 \\ min:0.76 \\ max:0.84} & \makecell{0.67 \pm 0.03 \\ min:0.60 \\ max:0.69} \\
     \hline
     \textbf{24} & QSAR biodegradation \cite{datasetqsar} &  \makecell{\textbf{0.81} \pm 0.01 \\ min:0.79 \\ max:0.84}  & \makecell{\textbf{0.64} \pm 0.05 \\ min:0.60 \\ max:0.71} & \makecell{0.78 \pm 0.02 \\ min:0.75 \\ max:0.82} & \makecell{\textbf{0.69} \pm 0.03 \\ min:0.67 \\ max:0.72} \\
     \hline
\end{tabular}
\caption{Analiza wpływu maksymalnej liczby atrybutów branej pod uwagę przy uczeniu klasyfikatorów.}
\label{tab:attr_table}
\end{table}
% \end{center}

Analizując wyniki z tabeli \ref{tab:attr_table}, widać że na zbadanych zbiorach danych nie obserwuje się znacznego wpływu maksymalnej liczby atrybutów na skuteczność algorytmu. Jest to optymistyczna obserwacja, ponieważ klasyfikatory należące do hybrydy były w stanie dobrze nauczyć się już na tylko kilku atrybutach. Może to być też spowodowane tym, że w lesie użyto $14$ klasyfikatorów. Można domyślać się, że gdyby klasyfikatorów było mniej, różnice w wynikach byłyby bardziej zauważalne.


\section{Aspekty techniczne projektu}
\subsection{Technologie wykorzystane w projekcie}
Projekt rozwijany był z użyciem \href{https://github.com/bartooo/random-forest-svm-hybrid}{repozytorium kodu na GitHub}. Implementacje wykonano z użyciem języka programowania \href{https://www.python.org}{Python 3.8.10} na systemie operacyjnym \href{https://ubuntu.com}{Ubuntu 20.04 LTS}. W celu implementacji operacji macierzowych i matematycznych wykorzystano \href{https://numpy.org}{bibliotekę NumPy}, do testów jednostkowych \href{https://docs.pytest.org/en/7.1.x/}{narzędzie PyTest}, a do formatowania kodu \href{https://github.com/psf/black}{narzędzie Black}.
% \newpage
\subsection{Struktura projektu}

Po rozpakowaniu folderu z projektem, ukazuje się następująca struktura katalogów i plików:

\begin{forest}
  for tree={
    font=\ttfamily,
    grow'=0,
    child anchor=west,
    parent anchor=south,
    anchor=west,
    calign=first,
    edge path={
      \noexpand\path [draw, \forestoption{edge}]
      (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.25pt] {} (.child anchor)\forestoption{edge label};
    },
    before typesetting nodes={
      if n=1
        {insert before={[,phantom]}}
        {}
    },
    fit=band,
    before computing xy={l=15pt},
  }
[ .
    [experiments/           \hspace{20mm} ← folder z eksperymentami 
        [*.py               \hspace{32mm} ← skrypty przeprowadzające eksperymenty]
        [logs/              \hspace{29.7mm} ← folder z logami
            [logs.log       \hspace{18mm} ← logi]
            [*.csv          \hspace{24.5mm} ← wyniki eksperymentów]
        ]    
    ]
    [data/                  \hspace{35mm} ← folder ze zbiorami danych]
    [src/                   \hspace{37mm} ← folder z kodem implementacji
        [SVM/               \hspace{31.9mm} ← folder z implementacją SVM 
            [*.py           \hspace{26.5mm} ← kod SVM]
            [tests/         \hspace{22.2mm} ← folder z testami SVM]
        ]
        [random\_forest/    \hspace{10mm} ← folder z implementacją lasu losowego
            [*.py           \hspace{26.5mm} ← kod lasu losowego]
            [tests/         \hspace{22.2mm} ← folder z testami lasu losowego]
        ]
    ]
    [main.py                \hspace{30.4mm} ← skrypt uruchamiający model]
    [main\_experiments.py   \hspace{4.5mm} ← skrypt uruchamiający eksperymenty]
    [requirements.txt       \hspace{11mm} ← moduły Python do zainstalowania]
]
\end{forest}
\newpage
\subsection{Instalacja}
W celu zainstalowania projektu i uruchomienia eksperymentów, należy wykonać następujące kroki:
\begin{enumerate}
    \item Uruchomić system operacyjny Ubuntu z zainstalowanym Pythonem i pobranym repozytorium.
    \item Przejść do folderu z projektem.
    \item Zainstalować niezbędne moduły języka Python:
    \begin{lstlisting}
    $ pip install -r requirements.txt
    \end{lstlisting}
    \item Uruchomić model hybrydowy z wybranymi hiperparametrami, przykładowo:
    \begin{lstlisting}
    $ python3 main.py --dataset breast_cancer --n_folds 5
        --num_classifiers 4 --tree_max_depth 4 
        --tree_min_entropy_diff 0.001 
        --tree_min_node_size 34 --svm_lambda 0.05 
    \end{lstlisting}
    \item Uruchomić eksperymenty porównujące modele:
    \begin{lstlisting}
    $ python3 main_experiments.py -WHAT models
    \end{lstlisting}
    \item Uruchomić eksperymenty porównujące wartości hiperparametrów:
    \begin{lstlisting}
    $ python3 main_experiments.py -WHAT parameters
    \end{lstlisting}
    \item Uruchomić testy jednostkowe:
    \begin{lstlisting}
    $ python3 -m pytest
    \end{lstlisting}
    
\end{enumerate}
\subsection{Testy}
W celu sprawdzenia czy implementacja nie zawiera błędów, działanie algorytmu sprawdzone zostało ręcznie - w tym celu zostały wykonane testy jednostkowe, które znajdują się w folderach tests/ w folderze src/. Poza sprawdzeniem działania podstawowych, logicznych elementów algorytmów, dodatkowo sprawdzono działanie przy małym, spreparowanym zbiorze danych.
\section{Podsumowanie}
W ramach wykonanego projektu nauczyliśmy się tworzyć modele, które cechuje poprawność i uniwersalność względem używanych danych, jak i pogłębiliśmy wiedzę odnośnie modeli SVM oraz Drzew Decyzyjnych. Udało nam się zauważyć, jak różnie można oceniać modele w zależności od stosowanej metryki, zestawu hiperparametrów jak i charakterystki zbioru danych. Zauważyliśmy, jak istotny jest dobór odpowiednich hiperparametrów do danego modelu oraz jak zróżnicowane są względem siebie poszczególne modele w uczeniu maszynowym. 
%--------------------------------------------
% Literatura
%--------------------------------------------
% \cleardoublepage % Zaczynamy od nieparzystej strony
\clearpage
\printbibliography


% %--------------------------------------------
% % Spisy (opcjonalne)
% %--------------------------------------------
% \pagestyle{plain}

% \vspace{0.8cm}

% \vspace{1cm}          % vertical space
% \listofappendicestoc  % Spis załączników

% % Załączniki
% \newpage
% \appendix{Nazwa załącznika 1}
% \appendix{Nazwa załącznika 2}
% \appendix{Nazwa załącznika 3}



\end{document}
