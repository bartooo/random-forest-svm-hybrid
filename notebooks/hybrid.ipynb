{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.SVM.SVM import SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth: int,\n",
    "        split_attribute_idx: int = None,\n",
    "        split_attribute_name: str = None,\n",
    "        split_threshold: float = None,\n",
    "        entropy: float = None,\n",
    "        samples: int = None,\n",
    "        values: np.ndarray = None,\n",
    "        label: int = None,\n",
    "        label_name: str = None,\n",
    "    ) -> None:\n",
    "        self.depth = depth\n",
    "        self.split_attribute_idx = split_attribute_idx\n",
    "        self.split_attribute_name = split_attribute_name\n",
    "        self.split_threshold = split_threshold\n",
    "        self.entropy = entropy\n",
    "        self.samples = samples\n",
    "        self.values = values\n",
    "        self.label = label\n",
    "        self.label_name = label_name\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "\n",
    "    def __str__(self):\n",
    "        node_str = f\"Depth: {self.depth}, \"\n",
    "        if self.split_attribute_name is not None:\n",
    "            node_str += f\"Split attribute: {self.split_attribute_name}, threshold: {self.split_threshold}, \"\n",
    "        elif self.split_attribute_idx is not None:\n",
    "            node_str += f\"Split attribute index: {self.split_attribute_idx}, threshold: {self.split_threshold}, \"\n",
    "        if self.label_name is not None:\n",
    "            node_str += f\"entropy: {self.entropy:.3f}, samples: {self.samples}, values: {self.values}, label: {self.label_name}\"\n",
    "        else:\n",
    "            node_str += f\"entropy: {self.entropy:.3f}, samples: {self.samples}, values: {self.values}, label: {self.label}\"\n",
    "        return node_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth: int,\n",
    "        min_entropy_diff: float,\n",
    "        min_node_size: int,\n",
    "        max_num_attributes: int = None,\n",
    "    ) -> None:\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_entropy_diff = min_entropy_diff\n",
    "        self.min_node_size = min_node_size\n",
    "        self.max_num_attributes = max_num_attributes\n",
    "\n",
    "    def _entropy(self, s: np.ndarray) -> float:\n",
    "        \"\"\"Calculates entropy of current dataset s\n",
    "        according to formula:\n",
    "        H(s) = sum(-p(x)*log2(p(x))),\n",
    "        where p(x) is the proportion of the number of elements\n",
    "        in class x to the number of elements in set s\n",
    "\n",
    "        Args:\n",
    "            s (np.ndarray): current dataset to calculate entropy on\n",
    "\n",
    "        Returns:\n",
    "            float: entropy of dataset s\n",
    "        \"\"\"\n",
    "        y = s[:, -1]\n",
    "        result = 0\n",
    "        counts = np.unique(y, return_counts=True)[1]\n",
    "        for count in counts:\n",
    "            proportion = count / y.shape[0]\n",
    "            result += -proportion * np.log2(proportion + 1e-5)\n",
    "        return result\n",
    "\n",
    "    def _find_split(\n",
    "        self, s: np.ndarray\n",
    "    ) -> Tuple[int, float, float, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Finds best attribute and threshold to split node on\n",
    "\n",
    "        Args:\n",
    "            s (np.ndarray): current dataset of node\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, float, float, np.ndarray, np.ndarray]: split attribute index, threshold, split entropy, left subset, right subset\n",
    "        \"\"\"\n",
    "        min_split_entropy = np.inf\n",
    "        best_split_attr_idx = None\n",
    "        best_threshold = None\n",
    "        best_left_subset = None\n",
    "        best_right_subset = None\n",
    "\n",
    "        possible_attributes = (\n",
    "            range(s.shape[1] - 1)\n",
    "            if self.max_num_attributes is None\n",
    "            else np.random.choice(\n",
    "                range(s.shape[1] - 1), size=self.max_num_attributes, replace=False\n",
    "            )\n",
    "        )\n",
    "        for attribute_idx in possible_attributes:\n",
    "            for threshold in np.unique(s[:, attribute_idx]):\n",
    "\n",
    "                left_subset = np.array(\n",
    "                    [row for row in s if row[attribute_idx] < threshold]\n",
    "                )\n",
    "                right_subset = np.array(\n",
    "                    [row for row in s if row[attribute_idx] >= threshold]\n",
    "                )\n",
    "\n",
    "                left_entropy = (\n",
    "                    (len(left_subset) / len(s)) * self._entropy(left_subset)\n",
    "                    if left_subset.shape[0] > 0\n",
    "                    else 0\n",
    "                )\n",
    "                right_entropy = (\n",
    "                    (len(right_subset) / len(s)) * self._entropy(right_subset)\n",
    "                    if right_subset.shape[0] > 0\n",
    "                    else 0\n",
    "                )\n",
    "\n",
    "                split_entropy = left_entropy + right_entropy\n",
    "                if split_entropy < min_split_entropy:\n",
    "                    min_split_entropy = split_entropy\n",
    "                    best_split_attr_idx = attribute_idx\n",
    "                    best_threshold = threshold\n",
    "                    best_left_subset = left_subset\n",
    "                    best_right_subset = right_subset\n",
    "\n",
    "        return (\n",
    "            best_split_attr_idx,\n",
    "            best_threshold,\n",
    "            min_split_entropy,\n",
    "            best_left_subset,\n",
    "            best_right_subset,\n",
    "        )\n",
    "\n",
    "    def _build_id3(\n",
    "        self,\n",
    "        dataset: np.ndarray,\n",
    "        depth: int,\n",
    "        orig_classes: np.ndarray,\n",
    "        attribute_labels: np.ndarray = None,\n",
    "        class_labels: np.ndarray = None,\n",
    "    ) -> Node:\n",
    "        if dataset.shape[0] == 0:\n",
    "            return None\n",
    "\n",
    "        X, y = dataset[:, :-1], dataset[:, -1].astype(\"int64\")\n",
    "\n",
    "        # all examples classified as one class\n",
    "        if np.unique(y).shape[0] == 1:\n",
    "            return Node(\n",
    "                depth=depth,\n",
    "                values=[y.tolist().count(c) for c in orig_classes],\n",
    "                entropy=0.0,\n",
    "                label=np.argmax([y.tolist().count(c) for c in orig_classes]),\n",
    "                label_name=class_labels[\n",
    "                    np.argmax([y.tolist().count(c) for c in orig_classes])\n",
    "                ]\n",
    "                if class_labels is not None\n",
    "                else np.argmax([y.tolist().count(c) for c in orig_classes]),\n",
    "                samples=dataset.shape[0],\n",
    "            )\n",
    "\n",
    "        # no attributes to split upon\n",
    "        if X.shape[1] == 0:\n",
    "            return Node(\n",
    "                depth=depth,\n",
    "                values=[y.tolist().count(c) for c in orig_classes],\n",
    "                entropy=self._entropy(dataset),\n",
    "                label=np.argmax([y.tolist().count(c) for c in orig_classes]),\n",
    "                label_name=class_labels[\n",
    "                    np.argmax([y.tolist().count(c) for c in orig_classes])\n",
    "                ]\n",
    "                if class_labels is not None\n",
    "                else np.argmax([y.tolist().count(c) for c in orig_classes]),\n",
    "                samples=dataset.shape[0],\n",
    "            )\n",
    "\n",
    "        (\n",
    "            best_split_attr_idx,\n",
    "            best_threshold,\n",
    "            min_split_entropy,\n",
    "            best_left_subset,\n",
    "            best_right_subset,\n",
    "        ) = self._find_split(dataset)\n",
    "\n",
    "        # decide about splitting\n",
    "        if (\n",
    "            depth < self.max_depth\n",
    "            and (self._entropy(dataset) - min_split_entropy) > self.min_entropy_diff\n",
    "            and (best_left_subset.shape[0] > self.min_node_size)\n",
    "            and (best_right_subset.shape[0] > self.min_node_size)\n",
    "        ):\n",
    "            root = Node(\n",
    "                depth=depth,\n",
    "                split_attribute_idx=best_split_attr_idx,\n",
    "                split_attribute_name=attribute_labels[best_split_attr_idx]\n",
    "                if attribute_labels is not None\n",
    "                else None,\n",
    "                split_threshold=best_threshold,\n",
    "                entropy=self._entropy(dataset),\n",
    "                samples=dataset.shape[0],\n",
    "                values=[y.tolist().count(c) for c in orig_classes],\n",
    "                label=np.argmax([y.tolist().count(c) for c in orig_classes]),\n",
    "                label_name=class_labels[\n",
    "                    np.argmax([y.tolist().count(c) for c in orig_classes])\n",
    "                ]\n",
    "                if class_labels is not None\n",
    "                else np.argmax([y.tolist().count(c) for c in orig_classes]),\n",
    "            )\n",
    "            root.left = self._build_id3(\n",
    "                best_left_subset,\n",
    "                depth=depth + 1,\n",
    "                orig_classes=orig_classes,\n",
    "                attribute_labels=attribute_labels,\n",
    "                class_labels=class_labels,\n",
    "            )\n",
    "            root.right = self._build_id3(\n",
    "                best_right_subset,\n",
    "                depth=depth + 1,\n",
    "                orig_classes=orig_classes,\n",
    "                attribute_labels=attribute_labels,\n",
    "                class_labels=class_labels,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            root = Node(\n",
    "                depth=depth,\n",
    "                entropy=self._entropy(dataset),\n",
    "                samples=dataset.shape[0],\n",
    "                values=[y.tolist().count(c) for c in orig_classes],\n",
    "                label=np.argmax([y.tolist().count(c) for c in orig_classes]),\n",
    "                label_name=class_labels[\n",
    "                    np.argmax([y.tolist().count(c) for c in orig_classes])\n",
    "                ]\n",
    "                if class_labels is not None\n",
    "                else np.argmax([y.tolist().count(c) for c in orig_classes]),\n",
    "            )\n",
    "\n",
    "        return root\n",
    "\n",
    "    def visualize(self) -> None:\n",
    "        queue = list()\n",
    "        queue.append(self.root)\n",
    "\n",
    "        while queue:\n",
    "            v = queue.pop(0)\n",
    "            print(v)\n",
    "            if v.left is not None:\n",
    "                queue.append(v.left)\n",
    "            if v.left is not None:\n",
    "                queue.append(v.right)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        attribute_labels: np.ndarray = None,\n",
    "        class_labels: np.ndarray = None,\n",
    "    ) -> None:\n",
    "        if attribute_labels is not None:\n",
    "            if attribute_labels.shape[0] != X.shape[1]:\n",
    "                raise Exception(\"Invalid shape of given attribute labels\")\n",
    "\n",
    "        if class_labels is not None:\n",
    "            if class_labels.shape[0] != np.unique(y).shape[0]:\n",
    "                raise Exception(\"Invalid shape of given class labels\")\n",
    "\n",
    "        if len(y.shape) == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        dataset = np.concatenate([X, y], axis=1)\n",
    "        self.root = self._build_id3(\n",
    "            dataset,\n",
    "            depth=0,\n",
    "            orig_classes=np.unique(y),\n",
    "            attribute_labels=attribute_labels,\n",
    "            class_labels=class_labels,\n",
    "        )\n",
    "\n",
    "    def _predict_sample(self, sample: np.ndarray) -> int:\n",
    "        current_node = self.root\n",
    "        current_prediction = current_node.label\n",
    "        while current_node.split_attribute_idx is not None:\n",
    "            if sample[current_node.split_attribute_idx] < current_node.split_threshold:\n",
    "                current_prediction = current_node.left.label\n",
    "                current_node = current_node.left\n",
    "            else:\n",
    "                current_prediction = current_node.right.label\n",
    "                current_node = current_node.right\n",
    "\n",
    "        return current_prediction\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        if len(X.shape) == 1:\n",
    "            X = np.expand_dims(X, axis=0)\n",
    "\n",
    "        if self.root is not None:\n",
    "            return np.array([self._predict_sample(sample) for sample in X])\n",
    "        else:\n",
    "            raise Exception(\"Decision Tree is not trained yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classifiers: int,\n",
    "        tree_max_depth: int,\n",
    "        tree_min_entropy_diff: float,\n",
    "        tree_min_node_size: int,\n",
    "        tree_max_num_attributes: int = None,\n",
    "    ):\n",
    "        self.tree_max_depth = tree_max_depth\n",
    "        self.tree_min_entropy_diff = tree_min_entropy_diff\n",
    "        self.tree_min_node_size = tree_min_node_size\n",
    "        self.tree_max_num_attributes = tree_max_num_attributes\n",
    "        self.num_classifiers = num_classifiers\n",
    "        self.classifiers = []\n",
    "\n",
    "    def _bootstrap_dataset(self, dataset: np.ndarray):\n",
    "        chosen_idxs = np.random.choice(\n",
    "            dataset.shape[0], size=dataset.shape[0], replace=True\n",
    "        )\n",
    "        return dataset[chosen_idxs]\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        attribute_labels: np.ndarray = None,\n",
    "        class_labels: np.ndarray = None,\n",
    "    ) -> None:\n",
    "        if len(y.shape) == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        dataset = np.concatenate([X, y], axis=1)\n",
    "        if self.tree_max_num_attributes > dataset.shape[1] - 1:\n",
    "            raise Exception(\"Invalid number of max attributes in Decision Tree\")\n",
    "\n",
    "        for i in range(self.num_classifiers):\n",
    "            boostrapped_dataset = self._bootstrap_dataset(dataset)\n",
    "            if i % 2 == 0:\n",
    "                tree = DecisionTree(\n",
    "                    max_depth=self.tree_max_depth,\n",
    "                    min_entropy_diff=self.tree_min_entropy_diff,\n",
    "                    min_node_size=self.tree_min_node_size,\n",
    "                    max_num_attributes=boostrapped_dataset.shape[1] - 1\n",
    "                    if self.tree_max_num_attributes is None\n",
    "                    else self.tree_max_num_attributes,\n",
    "                )\n",
    "                tree.fit(\n",
    "                    boostrapped_dataset[:, :-1],\n",
    "                    boostrapped_dataset[:, -1],\n",
    "                    attribute_labels=attribute_labels,\n",
    "                    class_labels=class_labels,\n",
    "                )\n",
    "                self.classifiers.append(tree)\n",
    "            else:\n",
    "                svm = SVM(\n",
    "                    lambd=0.05, minimizer_params={\"beta\": 0.01, \"min_epsilon\": 1e-20}\n",
    "                )\n",
    "                svm.fit(boostrapped_dataset[:, :-1], boostrapped_dataset[:, -1])\n",
    "                self.classifiers.append(svm)\n",
    "\n",
    "    def _predict_sample(self, sample: np.ndarray) -> int:\n",
    "        map_predictions = {1: 1, 0: 0, -1: 0}\n",
    "        return np.bincount(\n",
    "            np.squeeze(\n",
    "                [\n",
    "                    map_predictions[prediction]\n",
    "                    for classifier in self.classifiers\n",
    "                    for prediction in classifier.predict(sample)\n",
    "                ]\n",
    "            )\n",
    "        ).argmax()\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        if len(X.shape) == 1:\n",
    "            X = np.expand_dims(X, axis=0)\n",
    "\n",
    "        if len(self.classifiers) > 0:\n",
    "            return np.array([self._predict_sample(sample) for sample in X])\n",
    "        else:\n",
    "            raise Exception(\"Random Forest is not trained yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForest(\n",
    "    tree_max_depth=2,\n",
    "    tree_min_entropy_diff=0.01,\n",
    "    tree_min_node_size=50,\n",
    "    tree_max_num_attributes=3,\n",
    "    num_classifiers=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    attribute_labels=data.feature_names,\n",
    "    class_labels=data.target_names,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = random_forest.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90        36\n",
      "           1       0.94      0.92      0.93        50\n",
      "\n",
      "    accuracy                           0.92        86\n",
      "   macro avg       0.92      0.92      0.92        86\n",
      "weighted avg       0.92      0.92      0.92        86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2cfb67b4dfe5b36d779e8bb64d077441e0d2d0f10b697223d7298eaff1491342"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
