{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Lib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Problem definition\n",
    "### 3.1. Ogólnie\n",
    "Zadanie polega na znalezieniu funkcji $$ f(x)={w^{T}x-b}$$, która tworzy hiperpłaszczyznę zapewniającą klasyfikację (dopuszczającą pomyłki) z użyciem maszyny wektorów nośnych SVM. Otrzymana funkcja powinna zapewniać jak najmniejszą liczbę pomyłek przy klasyfikowaniu elementów zbioru BREAST CANCER do odpowiedniej klasy.\n",
    "\n",
    "Klasyfikacja odbywa się poprzez zwrócenie dla danego zestawu cech $x$ grupy $y(x) = -1 \\lor y(x) = 1$, do której należy za pomocą funkcji:\n",
    "\n",
    "$$\n",
    "y(x) =\n",
    "{\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "-1 & \\textrm{, $f(x) \\leq 0$}\\\\\n",
    "1 & \\textrm{, $f(x) > 0$}\n",
    "\\end{array}\n",
    "\\right\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "Aby otrzymać funkcję $f(x)$ należy znaleźć parametry $w$ i $b$, które minimalizują funkcję straty:\n",
    "$$ J(w,b)=\\Sigma_i(max(1-f(x_i)y_i, 0)) + \\lambda*||w||^2 $$\n",
    "\n",
    "Aby zoptymalizować owe parametry, zastosowana zostanie metoda gradientu prostego, w tym celu potrzebny będzie gradient funkcji $J(w,b)$:\n",
    "$$\n",
    "\\nabla J =\n",
    "\\begin{bmatrix}\n",
    "    \\partial J \\over \\partial w_1 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\partial J \\over \\partial w_n \\\\\n",
    "    \\partial J \\over \\partial b \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Natomiast pochodne cząstkowe te prezentują się następująco:\n",
    "$$\n",
    "{\\partial J \\over \\partial w_i}=\n",
    "{\\lambda*2*w_i} + \\Sigma_k(1 \\cdot\n",
    "{\\left\\{ \\begin{array}{ll}\n",
    "0 & \\textrm{, $ 1-f(x_k)y_k \\leq 0$ }\\\\\n",
    "-y_k \\cdot x_{k[i]} & \\textrm{, $ 1-f(x_k)y_k > 0$}\n",
    "\\end{array}\\right })\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "{\\partial J \\over \\partial b}=\n",
    "\\Sigma_k (1 * {\n",
    "\\left\\{ \\begin{array}{ll}\n",
    "0 & \\textrm{, $ 1-f(x_k) \\cdot y_k \\leq 0$ }\\\\\n",
    "y_k & \\textrm{, $ 1-f(x_k) \\cdot y_k > 0$}\n",
    "\\end{array}\\right\n",
    "}\n",
    ")\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Functions to train:\n",
    "+ funkcja **f(x)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, f_params):\n",
    "    b = f_params[-1]\n",
    "    w = f_params[:-1]\n",
    "    w = np.matrix(w)\n",
    "    x_mat = np.matrix(x)\n",
    "\n",
    "    if w.shape[0] != 1:  # w = horizontal vector\n",
    "        w = w.transpose()\n",
    "    if x_mat.shape[0] == 1: # x = vertical vector\n",
    "        x_mat = x_mat.transpose()\n",
    "\n",
    "    return (w*x_mat).item(0) - b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ gradient **J(w, b)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def grad_j(params, set_xs, set_ys, lambd, function_f):\n",
    "    \"\"\"\n",
    "    param params: vector of parameters ([w1,...,wn,b])\n",
    "    param set_xs: collection of parameteres of data for classification\n",
    "    param set_ys: collection of results of data for classification\n",
    "    param lambd: lambda used for SVM (λ)\n",
    "    param function_f: function of which the gradient is\n",
    "    \"\"\"\n",
    "    result = np.zeros(len(params))              # result is a numpy array of partial derivatives\n",
    "    # counting gradients for w1, w2, ..., wn\n",
    "    for iterator_w in range(len(params) - 1):   # partial derivatives for w1,..., wn\n",
    "        summ = 2*lambd*params[iterator_w]       # sum = 2 * λ * wi\n",
    "\n",
    "        for index, x in enumerate(set_xs):\n",
    "            y = set_ys[index]\n",
    "            distance = 1 - y* function_f(x, params)\n",
    "            if distance > 0:                    # if (1-f(xk)yk) > 0\n",
    "                summ -= y*x[iterator_w]         # sum = sum - yk*xk[i]\n",
    "        result[iterator_w] = summ\n",
    "\n",
    "# counting gradient for b\n",
    "    summ = 0                                    # sum = 0\n",
    "    for index, x in enumerate(set_xs):\n",
    "        y =set_ys[index]\n",
    "        distance = 1 - y* function_f(x, params)\n",
    "        if distance >0:                        # if (1-f(xk)yk) > 0\n",
    "            summ += y                          # sum += yk\n",
    "    result[-1] = summ\n",
    "    return result\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def grad_j(params, set_xs, set_ys, lambd, function_f):\n",
    "#     \"\"\"\n",
    "#     param params: vector of parameters ([w1,...,wn,b])\n",
    "#     param set_xs: collection of parameteres of data for classification\n",
    "#     param set_ys: collection of results of data for classification\n",
    "#     param lambd: lambda used for SVM (λ)\n",
    "#     param function_f: function of which the gradient is\n",
    "#     \"\"\"\n",
    "#     result_params = np.zeros_like(params)  # result is a numpy array of partial derivatives\n",
    "#     # counting gradients for w1, w2, ..., wn\n",
    "#     summs = params * 2 * lambd # sum = 2 * λ * wi\n",
    "#\n",
    "#\n",
    "#\n",
    "#     for index, x in enumerate(set_xs):\n",
    "#     #             y = set_ys[index]\n",
    "#     #             distance = 1 - y* function_f(x, params)\n",
    "#     #             if distance > 0:                    # if (1-f(xk)yk) > 0\n",
    "#     #                 summ -= y*x[iterator_w]         # sum = sum - yk*xk[i]\n",
    "#\n",
    "#     distances = 1 - set_ys * function_f(set_xs, params)\n",
    "#     summs[np.where(distances>0)] -= set_ys*set_xs # if (1-f(xk)yk) > 0: sum = sum - yk*xk[i]\n",
    "#     result_params[:-1] = summs\n",
    "#\n",
    "#\n",
    "#     summ = 0                                    # sum = 0\n",
    "#     for index, x in enumerate(set_xs):\n",
    "#         y =set_ys[index]\n",
    "#         distance = 1 - y* function_f(x, params)\n",
    "#         if distance >0:                        # if (1-f(xk)yk) > 0\n",
    "#             summ += y                          # sum += yk\n",
    "#     result[-1] = summ\n",
    "#     return result\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ algorytm realizujący metodę gradientu prostego:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(function_f, gradient_f, params, beta, set_xs, set_ys, lambd, max_steps=1000, min_epsilon = 1e-10):\n",
    "        \"\"\"\n",
    "        param function_f: function that is optimized\n",
    "        param gradient_f: gradient of function_f\n",
    "        param params: parameters to optimize\n",
    "        param beta: parameter beta used in gradient_descent\n",
    "        param set_xs: collection of parameters of data to classify\n",
    "        param set_ys: collection of targets of data to classify\n",
    "        param lambd: parameter used in SVM (λ)\n",
    "        params max_steps and min_epsilon: parameters for STOP CRITERIUM in gradient_descent\n",
    "        \"\"\"\n",
    "        new_param = params\n",
    "        act_step = 0\n",
    "        while 1:\n",
    "            act_gradient = gradient_f(new_param, set_xs, set_ys, lambd, function_f)\n",
    "            if np.linalg.norm(act_gradient) < min_epsilon or act_step > max_steps:\n",
    "                return new_param\n",
    "            new_param = new_param - beta * act_gradient\n",
    "            act_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Functions to evaluate:\n",
    "+ funkcja **y(x)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_y(x, function_f, params):\n",
    "    if function_f(x, params) <=0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_to_correct_targets(arr):\n",
    "    new_arr = copy.copy(arr)\n",
    "    new_arr[np.where(arr>0)]=1\n",
    "    new_arr[np.where(arr<=0)]=-1\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "y = change_to_correct_targets(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ dodatkowo zostaną zdefiniowane funkcje: trenujące model (**train_model()**) oraz wykonujące walidacje dla hiperparametru lambda(**validate_model()**)\n",
    "+ a także zostanie zdefiniowany zbiór możliwych lambd **lambdas = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5]**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5]\n",
    "\n",
    "def train_model(model0, training_set_x, training_set_y, param_lambda):\n",
    "    return gradient_descent(f, grad_j, model0, 0.001, training_set_x, training_set_y, param_lambda)\n",
    "\n",
    "def validate_model(training_set_x, training_set_y, validating_set_x, validating_set_y):\n",
    "    best_model = None\n",
    "    best_lambda = None\n",
    "    best_score = - math.inf\n",
    "    for param_lambda in lambdas:\n",
    "        model0 = [0 for _ in range(31)]\n",
    "        actual_model = train_model(model0, training_set_x, training_set_y, param_lambda)\n",
    "        results_validating = np.zeros(len(validating_set_y), dtype='int')\n",
    "        for ind, x in enumerate(validating_set_x):\n",
    "            results_validating[ind]=int(classify_y(x, f, actual_model))\n",
    "        n_of_successes = 0\n",
    "        for x, y in zip(results_validating, validating_set_y):\n",
    "            if x == y:\n",
    "                n_of_successes += 1\n",
    "        print(f\"Validating model with lambda: {param_lambda} gave score: {n_of_successes / len(results_validating)}\")\n",
    "        # as long as new score is not worse than actual best, lambda should be maximized\n",
    "        if (n_of_successes / len(results_validating) >= best_score):      \n",
    "            best_score = n_of_successes / len(results_validating)\n",
    "            best_lambda = param_lambda\n",
    "            best_model = actual_model\n",
    "    print(f\"Best lambda for this validation equals: {best_lambda} with score: {best_score}\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating model with lambda: 0.0001 gave score: 0.9651162790697675\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [75]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mvalidate_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [74]\u001B[0m, in \u001B[0;36mvalidate_model\u001B[1;34m(training_set_x, training_set_y, validating_set_x, validating_set_y)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m param_lambda \u001B[38;5;129;01min\u001B[39;00m lambdas:\n\u001B[0;32m     11\u001B[0m     model0 \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m31\u001B[39m)]\n\u001B[1;32m---> 12\u001B[0m     actual_model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_set_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_set_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_lambda\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m     results_validating \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mlen\u001B[39m(validating_set_y), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mint\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ind, x \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(validating_set_x):\n",
      "Input \u001B[1;32mIn [74]\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model0, training_set_x, training_set_y, param_lambda)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_model\u001B[39m(model0, training_set_x, training_set_y, param_lambda):\n\u001B[1;32m----> 4\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgradient_descent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_j\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_set_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_set_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_lambda\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [69]\u001B[0m, in \u001B[0;36mgradient_descent\u001B[1;34m(function_f, gradient_f, params, beta, set_xs, set_ys, lambd, max_steps, min_epsilon)\u001B[0m\n\u001B[0;32m     13\u001B[0m act_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 15\u001B[0m     act_gradient \u001B[38;5;241m=\u001B[39m \u001B[43mgradient_f\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_param\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mset_xs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mset_ys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlambd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_f\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mlinalg\u001B[38;5;241m.\u001B[39mnorm(act_gradient) \u001B[38;5;241m<\u001B[39m min_epsilon \u001B[38;5;129;01mor\u001B[39;00m act_step \u001B[38;5;241m>\u001B[39m max_steps:\n\u001B[0;32m     17\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m new_param\n",
      "Input \u001B[1;32mIn [67]\u001B[0m, in \u001B[0;36mgrad_j\u001B[1;34m(params, set_xs, set_ys, lambd, function_f)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, x \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(set_xs):\n\u001B[0;32m     15\u001B[0m     y \u001B[38;5;241m=\u001B[39m set_ys[index]\n\u001B[1;32m---> 16\u001B[0m     distance \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m y\u001B[38;5;241m*\u001B[39m \u001B[43mfunction_f\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m distance \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:                    \u001B[38;5;66;03m# if (1-f(xk)yk) > 0\u001B[39;00m\n\u001B[0;32m     18\u001B[0m         summ \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m*\u001B[39mx[iterator_w]         \u001B[38;5;66;03m# sum = sum - yk*xk[i]\u001B[39;00m\n",
      "Input \u001B[1;32mIn [66]\u001B[0m, in \u001B[0;36mf\u001B[1;34m(x, f_params)\u001B[0m\n\u001B[0;32m      3\u001B[0m w \u001B[38;5;241m=\u001B[39m f_params[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m      4\u001B[0m w \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmatrix(w)\n\u001B[1;32m----> 5\u001B[0m x_mat \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatrix\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m w\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:  \u001B[38;5;66;03m# w = horizontal vector\u001B[39;00m\n\u001B[0;32m      8\u001B[0m     w \u001B[38;5;241m=\u001B[39m w\u001B[38;5;241m.\u001B[39mtranspose()\n",
      "File \u001B[1;32mc:\\users\\01149762\\documents\\ai\\random-forest-svm-hybrid\\forest-svm-venv\\lib\\site-packages\\numpy\\matrixlib\\defmatrix.py:138\u001B[0m, in \u001B[0;36mmatrix.__new__\u001B[1;34m(subtype, data, dtype, copy)\u001B[0m\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m intype \u001B[38;5;241m!=\u001B[39m data\u001B[38;5;241m.\u001B[39mdtype:\n\u001B[0;32m    137\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m new\u001B[38;5;241m.\u001B[39mastype(intype)\n\u001B[1;32m--> 138\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m copy: \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnew\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    139\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m: \u001B[38;5;28;01mreturn\u001B[39;00m new\n\u001B[0;32m    141\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mstr\u001B[39m):\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = validate_model(X_train, y_train, X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success percent: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# def get_success_percent(model_results, official_results):\n",
    "#     sum = 0\n",
    "#     for x, y in zip(model_results, official_results):\n",
    "#         if x==y:\n",
    "#             sum+=1\n",
    "#     fraction = sum / len(model_results)\n",
    "#     print(f\"Success percent: {100*fraction}%\")\n",
    "#\n",
    "# get_success_percent(results_testing_41, testing_setosa_versicolor_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}